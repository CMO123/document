实在不想再看nfsd和nfs，听说lockd简单点，先看看那个，还有dlm, dlm好多.

nlm应该是network lock monitor, nsm应该是network state monitor. nlm使用nsm,nsm可以发现系统运行状态，主要是重新启动,告诉另一端的系统.

(fs/lockd/*) 这里全是nlm的服务端，那客户端在nfs中又实现一遍？可能lock协议是完全分布式的.
1. 数据结构
   
    nlm_lock
        * caller  / len  hostname之类的信息
        * nfs_fh
        * xdr_netobj    标志线程的信息
        * svid          包装pid
        * file_lock     多亏我看了file_lock

    nlm_cookie
        * char [NLM_MAXCOOKIELEN] / len  字符串

    nlm_args  这应该是nlm使用的参数
        * nlm_cookie
        * nlm_lock
        * block 阻塞？
        * reclaim   重新确认
        * state     状态
        * monitor   什么东西?
        * fsm_access
        * fsm_mode

    nlm_res
        * nlm_cookie lock操作中的确有许多cookie
        * status
        * nlm_lock

    nlm_reboot
        * mon / len 字符串
        * state
        * nsm_private

2. xdr操作 clnt4xdr.c
大部分都非常简单，但需要看一下
    a. encode_nlm4_holder / decode_nlm_holder (nlm_res)
        先看nlm4_holder
        * exclusive
        * svid 相当于pid，而且在decode_nlm_holder中也是这么做的
        * netobj
        * offset / len
    b. encode_nlm4_lock(nlm_lock)
        这里转换为nlm4_lock
        * caller_name
        * fh
        * oh
        * svid
        * off / len

应该对着协议看procedure的作用.

3. 数据结构， 大量的数据结构，也不应该看lockd,这么多东西！！ (lockd.h)
    nsm_private
        * char [SM_PRIV_SIZE]  16  nsm是什么?

    nsm_handle  这个用来做什么? 记录某个系统的信息?
        * list_head sm_link     在全局nsm_handles队列中使用
        * sm_count 
        * sm_mon_name / sm_name hostname
        * sockaddr_storage sm_addr  sm_addrlen
        * sm_monitored / sm_sticky
        * nsm_private sm_priv   里面是创建时间和nsm_handle的指针
        * sm_addrbuf            和sm_addr一样的信息

    nlm_host 这个表示那一端的?
        * hlist_node h_hash
        * sockaddr_storage h_addr / h_addrlen   server ip地址
        * sockaddr_storage h_srcaddr / h_srcaddrlen 
        * rpc_clnt h_rpcclnt    这是rpc client使用者
        * h_name                remote hostname
        * h_version
        * h_proto
        * h_reclaiming / h_server / h_noresvport / h_inuse  这是服务端和客户端都用的
        * wait_queue_head_t h_gracewait
        * rw_semaphore r_rwsec
        * h_state / h_nsmstate / h_pidcount / h_count 
        * h_mutext / h_nextrebind / h_expires
        * list_head h_lockowners 
        * h_lock
        * list_head h_granted / h_reclaim
        * nsm_handle h_nsmhandle
        * h_addrbuf

    nlm_rqst  这个应该是一个锁的请求,有些类似client的
        * a_count
        * a_flags
        * nlm_host a_host
        * nlm_args a_args
        * nlm_res a_res
        * nlm_block a_block
        * a_reties 
        * a_owner

    nlm_lookup_host_info
        * server (server for server or client) server 可能是nfs的server/client.锁都是从client向server请求,但server可以向client发送请求，释放锁.所以nfs client可能是lockd的server.
        * sockaddr sap / saplen     server地址
        * protocol
        * version
        * hostname / hostname_len   hostname
        * noresvport  使用non-priv端口
    这里重复的使用ip地址和hostname

操作：应该和上面的数据结构有关
越看越发现数据结构的复杂, 这里还是没弄清楚host到底代表那些东西. 不过我已经了解SUNRPC,所以这些都不是问题.
(fs/lockd/host.c)
1. nlm_alloc_host(nlm_lookup_host_info, nsm_handle)
    不清初nsm_handle和nlm_alloc_host是干什么的！创建一个nlm_host, 使用nlm_lookup_host_info填充它的成员变量, 不过不能枚举，太多了

2. nlm_destroy_host_locked(nlm_host)
    释放hash关系, 从它的关系中慢慢找出数据结构的意义
    -> nsm_unmonitor
    -> nsm_release(nlm_host->nsm_handle)
    -> rpc_shutdown_client(rpc_clnt)    关闭rpc_clnt
    -> kfree
    
3. nlmclnt_lookup_host(sockaddr, slen, protocol, version, hostname, noresvport)
    查找一个nlm_host,如果找不到就创建一个,找的应该是lockd client. 首先创建nlm_lookup_host_info. nlm_host在nlm_client_hosts hash表中，hash值使用sockaddr计算. 在nlm_client_hosts遍历hash链表，对比sockaddr,protocol, version.这三个相同，则说明找到. 否则创建一个，添加到nlm_client_hosts中.  这里有信息冗余hostname没有使用. lockd设计负责了，估计version也没有用.

4. nlmclnt_release_host(nlm_host)
    减少nlm_host->h_count，如果减到0，则释放它
    -> nlm_destroy_host_lockd.

5. nlmsvc_lookup_host(svc_rqst, hostname, hostname_len)
    找一个lockd server, 根据svc_rqst中的一些属性(svc_rqst->rq_addr, rq_proto, rq_vers) 这里为何就是svc_rqst? 在nlm_server_hosts hash表中，对比h_addr, h_srcaddr. 这里搞不清楚rq_daddr, rq_daddr.  svc_rqst包含请求者的信息
    -> nlm_gc_hosts 这里会自动回收不是用的nlm_host

6. nlmsvc_release_host(nlm_host)
    只是减小nlm_host->h_count，不用销毁nlm_host, nlm_gc_host回收

7. nlm_bind_host(nlm_host)
    创建rpc_clnt, 创建rpc_create_args,使用nlm_host的参数. 为何有着么多的sockaddr，还有servername。创建完成给nlm_host->h_rpcclnt. 如果nlm_host->h_rpcclnt不是空, 而且nlm_host->h_nextrebind时刻未到,使用rpc_force_rebind, 再调整h_nextrebind.
    -> rpc_create(rpc_create_args)

8. nlm_rebind_host(nlm_host)
    完成上面那个函数的一部分功能 
    -> rpc_force_rebind 调整nlm_host->h_nextrebind

9. nlm_get_host(nlm_host)
    增加计数,更新nlm_host->h_expires
    
10. next_host_state(hlist_head, nsm_handle, nlm_reboot)
    在hlist_head队列中查找一个nlm_host,它和nsm_handle关联,而且其nsmstate不是nfs_reboot->state,做一下修改:
    nlm_host->h_nsmstate = nlm_reboot->state. nlm_host->h_state ++
    这个主要在下面的函数中使用，是一个辅助遍历hash表的函数. 更新一下nlm_host的nsm状态.

11. nlm_host_rebooted(nlm_reboot)
    先找到nsm_handle，然后把和他关联的，而且状态不是nlm_reboot中状态的nlm_host都释放，包括client和server. 这个在sm_notify服务中使用,对client来说,unmonite,对server来说确认它没有锁文件. 这个还不清楚?
    遍历nlm_server_hosts,使用next_host_state做索引递增
    -> nlmsvc_free_host_resources(nlm_host) 释放打开的文件
    -> nlmsvc_release_host(host)
    遍历nlm_client_hosts,
    -> nlmclnt_recovery(nlm_host)   
    -> nlmclnt_release_host(nlm_host)   释放nsm信息？
    -> nsm_release 释放nsm_handle
    
12. nlm_shutdown_hosts()
    在模块卸载时使用，还有在lockd的server服务函数结束时使用,主要是释放nlm_server_hosts，而如果还有client,报warning
    遍历nlm_server_hosts，把所有nlm_host->h_expires改为jiffies-1, 释放其rpc_clnt
    -> rpc_shutdown_client(nlm_host->h_rpcclnt)
    -> nlm_gc_hosts()  回收工具

13. nlm_gc_hosts()
    回收所有nlm_server_hosts的nlm_host,这里应该是有优化的，先遍历一遍nlm_server_hosts，把nlm_host->h_insue设为0. 再遍历一遍,判断一下条件，不满足就不释放. 最后更新next_gc，下次回收的时间.
    -> nlm_host->h_count > 0  
    -> nlm_host->h_inuse 
    -> nlm_host->expires > jiffies
    释放时调用
    nlm_destroy_host_locked(nlm_host)


需要看一下工作队列 work_struct/workqueue_struct  kernel/workqueue.c

(fs/lockd/svc.c) 这应该lockd service实现的地方
1. lock_manager lockd_manager构造一个空的lock_manager,干什么呢？
grace_period_end 延时任务 DELAYED_WORK, 任务执行grace_ender
    全局变量grace_list, 它上面管理lockd_manager

2. grace_ender(work_struct)
    没什么有意义工作
    -> locks_end_grace(&lockd_manager)  把这个lockd_manager取出来
   set_grace_period 设置grace_period_end这个work_struct的触发延时, 它使用的回调函数是grace_ender,结束grace周期.

3. restart_grace()
    -> cancel_delayed_work_sync(grace_period_end)
    -> locks_end_grace(lockd_manager)
    -> nlmsvc_invalidate_all() 在另一个模块中
    -> set_grace_period() 设置grace时间

4. lockd(svc_rqst )
    这是lockd使用的内核线程, 循环处理svc接受的请求. 循环执行下面的过程
    -> set_grace_period() 为何要开启grace周期,这里不会动态关闭/创建线程?
    -> nlmsvc_retry_blocked() 据说是重新请求blocked请求
    -> svc_recv(svc_rqst, timeout)  接受rpc request
    -> svc_process 处理request
    如果线程需要退出,做清理工作
    -> flush_signals
    -> cancel_delayed_work_sync(grace_period_end)  结束grace周期,这个应该在一段时间后自动执行
    -> locks_end_grace(lockd_manager)
    -> nlmsvc_invalidate_all()  这是处理rpc server
    -> nlm_shutdown_hosts  关闭nlm_server_hosts.

5. create_lockd_listener(svc_serv, name, family, port)
    首先找svc_xprt, 使用svc_serv，这么大的结构， 还有name,family. 如果没找到，根据参数创建一个svc_xprt. svc_serv->sv_permsocks,管理所有的svc_xprt，他们监听服务请求. 这里的参数name是svc_xprt_class->xcl_name, family和port是服务使用的传输方式,所以这里要创建svc_xprt.(如果不了解SUNRPC server,打死都不知道要干什么)
    -> svc_find_xprt
    -> svc_create_xprt
    -> svc_xprt_put

6. create_lockd_family(svc_serv, family)
    建立svc_xprt, 建立传输层的东西. 对上面进行包装,lockd同时使用tcp/udp
    -> create_lockd_listener(svc_serv, "tcp/udp", family, nlm_udpport/nlm_tcpport)
    
7. make_socks(svc_serv)
    把ipv4,ipv6和tcp,udp都建立起来, 又是对上面进行包装.
    -> create_lockd_family(svc_serv, PF_INET/PF_INET6)

8. lockd_up()
    根据nlm服务创建rpc的server, svc_serv,然后创建链接层, 准备启动服务线程，处理rpc请求. 这里有一个全局变量nlmsvc_rqst,如果它有效，则说明lockd已经启动,这里只有一个svc_rqst,说明lockd使用单线程.
    -> svc_create(nlmsvc_program, LOCKD_BUFSIZE, NULL)
    -> make_socks(svc_serv) 然后创建svc_xprt, 这里倒是创建4个监听的.
    -> svc_prepare_thread(svc_serv, pool, NUMA_NO_NODE) 创建svc_rqst给nlmsvc_rqst
    -> svc_sock_update_bufs 在nfsd中没印象这么多操作！
    -> kthread_run(lockd, nlmsvc_rqst, svc_serv->sv_name) 这里手动开始执行lockd进程,看来这是一个模块初始化函数
    -> svc_destroy(svc_serv) 如果失败的话,释放创建的svc_serv

9. lockd_down
    通过计数决定lockd是否还需要运行 nlmsvc_users. nlmsvc_task就是nlmsvc_rqst使用的线程.
    -> kthread_stop(nlmsvc_task)
    -> svc_exit_thread(nlmsvc_rqst)

上面介绍lockd内核线程的创建和服务过程，只说了它如何利用SUNRPC server创建其服务，而且它做的东西也在其他文件中,这里还是把其他模块的接口整合一下.

(fs/lockd/grace.c)
这个是lock相关操作，应该和file_lock相关, 这里管理lock_manager，但没有看到哪里调用它注册的回调函数?
1. locks_start_grace(lock_manager)
    lock_manager就是包装一个list_head, 此函数在linux/fs.h中，如果没有lockd，别人用怎么办? 把lock_manager添加到grace_list队列中，这个队列是lockd全局的.

2. locks_end_grace(lock_manager)
    把lock_manager从grace_list中删除

3. locks_in_grace()
    grace_list非空

完了！！

lockd还是分了server和client两个部分.server的功能应该是接受请求，根据请求中的锁向文件加锁,如果成功返回结果，如果不成功放到nlm_blocked队列中,然后让nlmsvc_rqst线程执行时处理队列的东西.
(fs/lockd/svclock.c)
数据结构
    nlm_file
        * hlist_node
        * nfs_fh    f_handle
        * file      f_file
        * nlm_share f_shares
        * list_head f_blocks
        * f_locks / f_count / f_mutex

    nlm_block  被拒绝的可block的锁，使用nlm_block维护，这应该是client, 为何有svc_serv?这是server的东西！！ 用kref是不是浪费?
        * kref b_count
        * list_head b_list      nlm_blocked队列使用的成员变量
        * list_head b_flist     nlm_file->f_blocks使用的队列
        * nlm_rqst b_call       这个不是svc_rqst! 
        * svc_serv b_daemon     这是全局唯一的
        * nlm_host b_host       这个host是锁的请求者?
        * b_when / b_id / b_granted 
        * nlm_file b_file
        * cache_req b_cache_req
        * file_lock b_fl
        * cache_deferred_req   b_deferred_req
        * b_flags

    nlm_rqst  这个应该是一个锁的请求信息, 在client/server都使用.
        * a_count
        * a_flags
        * nlm_host a_host
        * nlm_args a_args
        * nlm_res a_res
        * nlm_block a_block
        * a_reties 
        * a_owner

1. nlm_block队列管理
    a. nlmsvc_insert_block_locked(nlm_block, when)  / nlmsvc_insert_block(nlm_block, when)
        nlm_block怎么是这么复杂的数据结构? 把nlm_block添加到nlm_blocked队列中，按nlm_block->b_when排序. 后一个函数使用锁nlm_blocked_lock封装前一个函数. 如果nlm_block已经在队列中，会改变它在队列中的位置.  如果它不在队列中: kref_get.

    b. nlmsvc_remove_block(nlm_block)
        把它从nlm_blocked队列中释放，注销它
        -> nlmsvc_release_block   kref_put

    c. nlmsvc_lookup_block(nlm_file, nlm_lock)
        找和nlm_file/nlm_lock对应的nlm_block. 这样不大准确吧？整个比较过程没有hostname/sockaddr之类的信息, 难道nlm_file是属于某一nlm_host? 遍历nlm_blocked上的所有nlm_block,比较nlm_block->nlm_file和nlm_block->nlm_rqst->nlm_lock->nlm_args->nlm_lock->file_lock， 递增nlm_block->b_count
        -> nlm_compare_locks(file_lock, file_lock)

    d. nlm_cookie_match(nlm_cookie, nlm_cookie)
        比较两个字符串

    e. nlmsvc_find_block(nlm_cookie)
        遍历nlm_blocked，找到一个nlm_block，nlm_block->nlm_rqst->nlm_lock->nlm_cookie相同
        -> nlm_cookie_match

nlm_block 这个数据结果用来处理LOCK请求,来了之后如果不能立即处理,把请求存起来，使用nlm_rqst记录信息,当请求处理完成后,使用GRANT请求返回.
    a. nlmsvc_create_block(svc_rqst, nlm_host, nlm_film, nlm_lock, nlm_cookie)
        创建一个nlm_block, 需要使用nlm_rqst. nlm_host是锁请求者, 但是这个nlm_rqst应该是向client发送GRANT请求使用的, nlm_rqst->nlm_args->nlm_lock->caller是localhost.而其他nlm_lock的内容不变.
        -> nlm_alloc_call(nlm_host)  
        -> nlmsvc_setgrantargs(nlm_rqst, nlm_lock) 为GRANT请求准备nlm_rqst.
        nlm_rqst->nlm_args->nlm_lock->file_lock->fl_lmops = nlmsvc_lock_operation，当server想本地文件加锁时,回调处理函数使用特制的函数.
        -> nlm_client_next_cookie(nlm_rqst->nlm_args->nlm_cookie)  nlm_cookie就是server的一个计数.
        初始化nlm->block 
            b_daemon = rpc_rqst->svc_serv
            b_host = nlm_host
            b_file = nlm_file
            b_fl = NULL 这里竟然不用，只在GETLK时使用
            nlm_block->f_flist <> nlm_file->f_blocks
            b_call = nlm_rqst
            nlm_rqst->a_block = nlm_block
        全是一个锁和地址的操作，还这么麻烦！
        -> nlmsvc_release_call(nlm_rqst) 如果失败，就释放创建的nlm_rqst.

    b. nlmsvc_unlink_block(nlm_block)
        把nlm_block使用的file_lock从某个file_lock的等待队列上取下来, 同时把它从nlm_blocked队列上取下来.
        -> posix_unblock_lock(nlm_block->nlm_file->file, nlm_block->nlm_rqst->nlm_args->nlm_lock->file_lock) 把等待的file_lock从等待队列中取出来
        -> nlm_remove_block(nlm_block)

    c. nlmsvc_free_block(kref)
        回调释放函数, 从nlm_file->f_blocks队列中释放(nlm_block->b_flist), 释放nlm_block->file_lock,还有自己
        -> nlmsvc_freegrantargs(nlm_block->nlm_rqst)
        -> nlmsvc_release_call(nlm_block->nlm_rqst)  释放file_lock->private数据
        -> nlm_release_file(nlm_block->nlm_file)

    d. nlmsvc_release_block(nlm_block)
        -> kref_put(nlm_block->b_count, nlmsvc_free_block)

下面是锁的处理,还有不沾边的东西。。
    a. nlmsvc_traverse_blocks(nlm_host, nlm_file, nlm_host_match_fn_t)
        This function release nlm_file's nlm_block which belong to nlm_host, nlm_host_match_fn_t judge which one can be released. nlm_host_match_fn_t receive two nlm_host, it just compares, or check nlm_host's sub variety.
        -> nlmsvc_unlink_block(nlm_block)
        -> nlmsvc_release_block(nlm_block)

    b. nlmsvc_setgrantargs(nlm_rqst, nlm_lock)
        根据nlm_block设置nlm_rqst, 为GRANTED操作准备
        -> nlm_rqst->nlm_args->nlm_lock->file_lock = nlm_lock->file_lock
        -> nlm_rqst->nlm_args->nlm_lock->nfs_fh = nlm_lock->nfs_fh
        -> nlm_rqst->nlm_args->nlm_lock->caller = utsname()->nodename 本地域名？
        -> nlm_rqst->nlm_args->nlm_lock->xdr_object = nlm_rqst->a_owner这是什么?
        -> nlm_rqst->nlm_args->nlm_lock->svid = nlm_lock->file_lock->pid
        -> nlm_rqst->nlm_args->nlm_lock->oh = nlm_lock->oh

    c. nlmsvc_freegrantargs(nlm_rqst)
        -> locks_release_private(nlm_rqst->nlm_args->nlm_lock->file_lock) file_lock中带的private是什么？ 就是file_lock->u.lockowner之类的信息

    d. nlmsvc_defer_lock_rqst(svc_rqst, nlm_block)
        当nlm_block请求失败是,把请求的数据放到deferred_cache_req中.
        -> nlmsvc_insert_block(nlm_block, NLM_TIMEOUT)
        -> nlm_block->b_cache_req = nlm_rqst->rq_chandle 这算什么？
        -> nlm_rqst->rq_chandle->defer()  这个defer好象是svc_defer.看来这里要把这个请求缓存起来,而不是立即返回结果.

    e. nlmsvc_lock(svc_rqst, nlm_file, nlm_host, nlm_lock, wait, nlm_cookie, reclaim)
        这个函数在proc function函数中调用. 处理LOCK请求. 先根据参数查找nlm_block,如果找不到创建一个. 如果nlm_block->b_flags中包含B_QUEUED(锁已经存在)，检查它的标志.
        * nlm_block->b_granted表示是否已经获取锁，如果已经获取，则把它从nlm_blockd队列释放. 
        * 如果nlm_block->b_flags表示B_TIMED_OUT,则返回错误,同样把它从nlm_blockd中释放. 
        要添加一个新的nlm_block, 检查reclian和grace是否一致.再看锁没有在queue中，需要对文件加锁, 如果不需要等待，则去掉nlm_lock->file_lock.fl_flags中的FL_SLEEP(刚看了以下，如果FL_SLEEP,会把file_lock放到等待队列中)
        -> nlmsvc_unlink_block
        -> vfs_lock_file(nlm_file->file, F_SETLK, file_lock, NULL) 检查结果，对于EGAIN和FILE_LOCK_DEFERED,都可以block，等一段时间再锁. 如果wait=1,则把请求信息放到deferred_cached_req中
        -> nlmsvc_insert_block(nlm_block, NLM_NEVER)为何这里使用never，如果LOCK_MSG或许就设置延时一段时间，如果不是，为何不下次再重新创建？
        -> nlmsvc_release_block(nlm_block)为何又释放

    e. nlmsvc_testlock(svc_rqst, nlm_file, nlm_host, nlm_lock, nlm_lock conflock, nlm_cookie)
        如果被阻塞,只会延时处理这个请求，不会向nlm_blocked队列中添加新的nlm_block, 其他错误返回. 如果返回正确有两种情况,一种是可以加锁,另一种是返回冲突锁的信息.
        先根据nlm_file和nlm_lock找一个相同的锁，当然是锁的type/off/end等一样,如果没有就创造一个.  如果找到一个nlm_block，而且状态是B_QUEUED,检查其状态，如果B_TIMED_OUT则返回错误. 如果是B_GOT_CALLBACK,表示这个锁要不就是granted,要不就是TIMEOUT. 但这里要看nlm_block中锁的类型,如果是UNLCK,说明新加锁是不能成功的,UNLCK已经成功还没有告诉client, 其他情况返回granted,说明之前家的一个锁已经成功,再加一个也没问题.
        -> vfs_test_lock(nlm_file->file, nlm_lock->file_lock) 如果返回FILE_LOCK_DEFERED，则延时处理锁,为何这里就不判断是否请求是wait的? 注意这里即使放回0,也要检查file_lock. 如果file_lock->fl_type == F_UNLCK,表示可以加锁,否则返回冲突的锁信息. 
        -> nlmsvc_defer_lock_rqst(svc_rqst, nlm_block)
        这里需要注意vfs_test_lock没有返回值，如果可以加锁，则file_lock->fl_type变成F_UNLCK, 否则冲突的锁信息在file_lock中，把它拷贝到conflock中.
        -> nlmsvc_release_block(nlm_block)释放老的或创建的nlm_block

    f. nlmsvc_unlock(nlm_file, nlm_lock)
        对应CANCEL操作, 取消nlm_block, 解锁,返回结果是正确或无锁可解.
        -> nlmsvc_cancel_blocked(nlm_file, nlm_lock)
        -> vfs_lock_file(nlm_file->file, F_SETLK, nlm_lock->file_lock, NULL)

    g. nlmsvc_cancel_blocked(nlm_file, nlm_lock)
        同时会取消nlm_block和file_lock
        -> nlmsvc_findup_block(nlm_file, nlm_file)
        -> vfs_cancel_lock(nlm_block->nlm_file->file, nlm_block->nlm_rqst->nlm_args->nlm_lock->file_lock) 这个函数还真是少用
        -> nlmsvc_unlink_block(nlm_block)
        -> nlmsvc_release_block

下面是lock_manager_operations的回调函数
    a. nlmsvc_update_deferred_block(nlm_block, file_lock, result)
        在下面的函数中调用,添加nlm_block->b_flags的B_GOT_CALLBACK, 如果result=1,说明nlm_block加锁成功, 否则是B_TIMED_OUT, 把file_lock拷贝到nlm_block->b_fl. 

    b. nlmsvc_grant_defered(file_lock, file_lock, result)
        这是lm_grant回调函数，只有fuse和dlm中也使用它. 找一个nlm_block,更新它的状态，结果在result和file_lock中.它和file_lock相同(owner/pid/start/end/type)，
        -> nlm_compare_locks(file_lock, file_lock) 这个找法不确定,如果有多个锁等待,如何保证他们的信息不想同? 不过这里的遍历是找到一个就返回.
        -> nlmsvc_update_defered_block(nlm_block, lock_file, result) revoke above function, if nlm_block is B_QUEUED. and nlm_block could not be B_QUEUED, and set nlm_block->b_granted=1 if result = 0.
        -> nlmsvc_insert_block_locked(nlm_block, 0) insert nlm_block into nlm_blockd as head.
        -> svc_wake_up(nlm_block->svc_serv) 唤醒svc_rqst,让它处理nlm_block.

    c. nlmsvc_notify_blocked(file_lock)
        this is callback function as lm_notify. (check locks.c, it will replace wake_up(file_lock->wait_queue_head_t)). some file_lock is dead, file_block (which is blocked by that file_lock) is waked up. Traverse nlm_blocked(all nlm_block), find the matching nlm_blocked, adjuest it's pos to head of nlm_blocked, and wake up svc_serv
        -> nlmsvc_insert_block_locked(nlm_block, 0) 这里和上面的重新入队操作都是放在对首.
        -> svc_wake_up(nlm_block->svc_serv)

    d. nlmsvc_same_owner(file_lock, file_lock)
        check if fl_owner and fl_pid are both equal. What's the mean of pid?  骗子啊,这里的fl_owner竟然可以是nlm_host.

    e. lock_manager_operation nlmsvc_lock_operations 
        -> lm_compare_owner = nlmsvc_same_owner
        -> lm_notify = nlmsvc_notify_blocked
        -> lm_grant = nlmsvc_grant_deferred

下面是处理nlm_block和deferred_cache_req,同时向client发送GRANTED请求
    a. nlmsvc_grant_blocked(nlm_block)
        重新处理nlm_block, 如果它已经granted,或者重新加锁成功, retry grant callback
        -> kref_get(nlm_block->b_count)   #correspond nlmsvc_release_block
        -> nlm_rebind_host(nlm_block->nlm_host) 
        -> vfs_lock_file(nlm_file->file, F_SETLK, nlm_lock->file_lock, NULL) lock file again, and add FL_SLEEP to fl_flags, why? check result, 
        if return FILE_LOCK_DEFERRED, wait again until file_lock wake up it.
            -> nlmsvc_insert_block(nlm_block, NLM_NEVER) 一直等待,知道有其他锁唤醒它
            -> nlm_release_block
        if return error, wait for 10s
            -> nlmsvc_insert_block(nlm_block, 10*HZ)
        what if return ok or granted
            -> nlmsvc_insert_block(nlm_block, NLM_NEVER) 反正要发送了,发送的回调函数应该会释放它.
            -> nlm_async_call(nlm_block->nlm_rqst, NLMPROC_GRANTED_MSG, &nlmsvc_grant_ops)  yes, callback proc. 注意这些rpc_call_ops,当发送完成后调用的函数在下面

    b. nlmsvc_grant_callback(rpc_task, void)
        this is callback function from rpc, just see its parameter. rpc call maybe timeout or success, so what? 这个函数好懒，还是调整nlm_block在nlm_blocked中的位置,让lockd处理它.
        -> nlmsvc_insert_block_locked(nlm_block, timeout)
        -> svc_wake_up(nlm_block->svc_serv)

    c. nlmsvc_grant_release(void)
        -> nlm_release_block(nlm_rqst->nlm_block)  If a lock is granted, is there a corresponding nlm_block? No!!

    d. nlmsvc_grant_reply(nlm_cookie, status)
        This is to deal GRANT_RES request. find a nlm_block according to nlm_cookie, and modify its state. 发送GRANT消息之后, 收到GRANT_RES消息，反馈GRANT的结构,如果grace_period错误,等会再发送，否则删除这个nlm_block,无论它要不要. 锁的信息已经在inode中了.
        -> nlmsvc_find_block(nlm_cookie)
        -> nlmsvc_insert_block(nlm_block, 10*HZ) if status = nlm_lck_denied_grace_period.  nfs client has grace period?  we try send GRANT_MS again?
        -> nlmsvc_unlink_block(nlm_block)  client may accept or reject the lock, so remove nlm_block, just remove??  YES
        -> nlmsvc_releas_block(nlm_block)

    e. retry_deferred_block(nlm_block)
        wooo, retry a deferred block, and something with cache_deferred_req? this is a helper function.  在lockd中处理,使用deferred_cache_req，不大清楚这个东西..
        -> nlmsvc_insert_block(nlm_block, NLM_TIMEOUT)
        -> nlm_block->cache_deferred_req->revisit(...)  *** tough things
 
    f. nlmsvc_retry_blocked()
        this function is revoked from lockd kthread, retry all blocked locks that have been notified. grant notification should be retransimitted. Traverse nlm_blocked, 这是一个nlm_block->b_list组成的队列, check all nlm_block whose b_when is earlier than jiffies. 综合上面的两个函数,lockd并不简单的接受数据,还要处理延时的.
        -> retry_deferred_block 如果nlm_block->b_flags包含B_QUEUED. 执行被延时的请求
        -> nlmsvc_grant_blocked(nlm_block) 检查nlm_block->b_granted,如果获得锁,发送授权rpc request, 否则重新给文件加锁,如果加锁成功,同样回发rpc.如果锁被阻塞,把nlm_block放到nlm_blocked队列. 如果出错也放回队列，但他们在队列中的等待时间不一样. 睡眠时是永久等待,至少不会在这个函数中处理它,否则等待7HZ

脑袋好乱，不想看这些东西了，有空整理一下流程就散了吧..看完下面的再看别的,散了，还是有空再看吧...
nlm_file (fs/lockd/svcsubs.c)
hash table : nlm_files , hash value is based on nfs_fh.
1. nlm_lookup_file(svc_rqst, nlm_file, nfs_fh)
    find a nlm_file, if fail, create a new one. There is little info in nlm_file, two list(hash list, block list), file, nfs_fh, count.  If find a matching nlm_file, increase nlm_file->f_count.
    -> nlmsvc_ops->fopen(svc_rqst, nfs_fh, nlm_file->file) open file

2. nlm_delete_file(nlm_file)
    remove nlm_film from nlm_files hash table, and close file, free memory.
    -> nlmsvc_ops->fclose(nlm_file->file)

3. nlm_traverse_locks(nlm_host, nlm_file, nlm_host_match_fn_t)
    unlock nlm_file->file. And nlm_host_match_fn_t decide who will be unlocked. It may compare wo nlm_host, or check one's variety.
    -> vfs_lock_file(nlm_file->file, F_SETLK, {F_UNLCK...})

4. nlm_inspect_file(nlm_host, nlm_file, nlm_host_match)
    inspect a single file??
    -> nlmsvc_traverse_blocks(nlm_host, nlm_file, match)
    -> nlmsvc_traverse_shares(nlm_host, nlm_file, match)
    -> nlm_traverse_locks(nlm_host, nlm_file, nlm_host_match_fn_t) this call will check nlm_file's all file_lock, and if there is some nlm_file whose nlm_host is equal parameter nlm_host, then nlm_host will try to unlock the nlm_file. If unlock successes, then return ok, else return 1.

5. nlm_file_inuse(nlm_file)
    check nlm_file->f_count, nlm_file->f_blocks, nlm_file->file->f_path->dentry->d_inode->i_flock have some file_lock with fl_lmops equal to nlmsvc_lock_operations

6. nlm_traverse_files(data, nlm_host_match_fn_t, is_failover_file())
    traverse nlm_files hash table, there are twice judgement. The first one is 'is_failover_file', it decides which one will be 'inspected'; and in 'inspecting', nlm_file->nlm_host will be checked.
    -> nlm_inspect_file() check block of nlm_file.
    -> nlmsvc_ops->fclose()  If there is no block on this nlm_file and unblocked locks, just close and destroy it.

7. nlm_release_file(nlm_file)
    release nlm_file, decrease nlm_file->f_count and delete it when f_count = 0 and it isn't in use.
    -> nlm_file_inuse(nlm_file)
    -> nlm_delete_file(nlm_file)

8. nlmsvc_mark_host(data, nlm_host dummy)
    helper function for resource traversal  data is type of nlm_host, set nlm_host->h_inuse=1

9. nlmsvc_same_host(data, nlm_host dummy)
    check whether data is equal to dummy...

9. nlmsvc_is_client(data, nlm_host)
    if data(nlm_host) is used as server, set somthing as sm, nlm_host->nlm_handle->sm_sticky = 1. nlm_host->h-server decide whether it is server or client.

10. nlmsvc_mark_resources()
    -> nlm_traverse_files(NULL, nlmsvc_mark_host, NULL). As nlmsvc_mark_host is used as callback function and return bool value as condition. But it modify nlm_host in fact. So it could be body of such loop.
 
11. nlmsvc_free_host_resources(nlm_host)
    -> nlm_traverse_files(nlm_host, nlmsvc_same_host, NULL) nlm_traverse_files is kind of bigger alibity. It traverse all nlm_files, and match function works all nlm_file, and its return could be return to nlm_traverse_files' caller.

12. nlmsvc_invalidate_all()
    -> nlm_traverse_files(NULL, nlmsvc_is_client, NULL) this will effect nlm_files associated with lots nlm_host.

13. nlmsvc_match_sb(data, nlm_file)
    data is kind of super_block, compare nlm_file->file->f_path.dentry->d_sb to data

14. nlmsvc_unlock_all_by_sb(super_block)
    -> nlm_traverse_files(super_block, nlmsvc_always_match, nlmsvc_match_sb) Here the second function decide which nlm_file will be 'inspected'.

15. nlmsvc_match_ip(data, nlm_host)
    rpc_cmp_addr(nlm_host->h_srcaddr, data)   nlm_host->h_srcaddr should be server's ip.
    
16. nlmsvc_unlock_all_by_ip(sockaddr)
    -> nlm_traverese(files(sockarr, nlmsvc_match_ip, NULL)  Here use nlm_host compare.

(fs/lockd/svc4proc.c)
Lockd server procedures, there is no async procedures, and there is no NLM_*_RES
1. nlm4svc_retrieve_args(svc_rqst, nlm_args, nlm_host, nlm_file)
    retrieve nlm_host by nlm_args->nlm_lock->caller, and check whether it is monitored to satisfy the need of nlm_args->monitor.
    retrieve nlm_file by nlm_args->nlm_lock->nfs_fh.
    If all is ok, update nlm_args->nlm_file->file_lock->{file,fl_owner=host,fl_lmops}

2. nlm4svc_proc_null(svc_rqst, argp, resp)
    NULL procedure.

3. nlm4svc_proc_test(svc_rqst, nlm_args, nlm_res)
    This is for TEST procedure, check for conflicting lock.
    -> nlm4svc_retrieve_args(nlm_args, nlm_host, nlm_file) 
    -> nlmsvc_testlock(svc_rqst, nlm_file, nlm_host, nlm_lock, nlm_lock/output, nlm_cookie)
    -> nlmsvc_release_host
    -> nlm_release_file

4. nlm4svc_proc_lock(svc_rqst, nlm_args, nlm_res)
    LOCK procedure  Just copy from above
    -> nlm4svc_retrieve_args(nlm_args, nlm_host, nlm_file) 
    -> nlmsvc_lock(svc_rqst, nlm_file, nlm_host, nlm_args->nlm_lock, isblocked, nlm_args->nlm_cookie, nlm_args->reclaim)
    -> nlmsvc_release_host
    -> nlm_release_file

5. nlm4svc_proc_cancel(svc_rqst, nlm_args, nlm_res)
    CANCEL, check grace time 
    -> locks_in_grace()
    -> nlm4svc_retrieve_args(nlm_args, nlm_host, nlm_file) 
    -> nlmsvc_cancel_blocked(nlm_file, nlm_args->nlm_lock)
    -> nlmsvc_release_host
    -> nlm_release_file
    
6. nlm4svc_proc_unlock(svc_rqst, nlm_args, nlm_res)
    UNLOCK
    -> locks_in_grace()
    -> nlm4svc_retrieve_args(nlm_args, nlm_host, nlm_file) 
    -> nlmsvc_unlock(nlm_file, nlm_args->nlm_lock)
    -> nlmsvc_release_host
    -> nlm_release_file

7. nlm4svc_proc_granted(svc_rqst, nlm_args, nlm_res)
    -> nlmclnt_grant(svc_rqst->rq_addr, nlm_args->nlm_lock)  This is used as lock client. And there is nothing left except return rpc_success

8. nlm4svc_callback_exit(rpc_task, data)
    rpc callback function, why rpc has callback?

9. nlm4svc_callback_release(data)
    -> nlmsvc_release_call(data)  release nlm_rqst, what's nlm_rqst?

    rpc_call_ops nlm4svc_callback_ops  callback functions?

10. nlm4svc_callback(svc_rqst, proc, nlm_args, func)
    func is some rpc call.
    -> nlmsvc_lookup_host(svc_rqst, nlm_args->nlm_lock.call ...)
    -> nlm_alloc_call(nlm_host) create a nlm_rqst
    -> revoke func, result is put to nlm_host->nlm_args
    -> nlm_async_reply(nlm_rqst, proc, nlm4svc_callback_ops)

11. nlm4svc_proc_test_msg(svc_rqst, nlm_args)
    -> nlm4svc_callback(svc_rqst, NLMPROC_TEST_RES, nlm_args, nlm4svc_proc_test)
    ok, this is a procedure, is revoked when receiving NLMPROC_TEST_MSG, so it direct callback NLMPROC_TEST_RES, of course it will first execute TEST procudure. and after starting async rpc call, return rpc_success for NLMPROC_*_MSG call.

12. nlm4svc_proc_lock_msg()
    -> NLMPROC_LOCK_RES

12. nlm4svc_proc_cancel_msg()  
    -> NLMPROC_CANCEL_RES

13. nlm4svc_proc_unlock_msg()
    -> NLMPROC_UNLOCK_RES

14. nlm4svc_proc_granted_msg
    -> NLMPROC_GRANTED_RES

15. nlm4svc_proc_sm_notify(svc_rqst, nlm_args, res)
    -> nlm_privileged_requester(svc_rqst)
    -> nlm_host_rebooted(nlm_args)

16. nlm4svc_proc_granted_res(svc_rqst, nlm_res, void)
    -> nlmsvc_grant_reply(nlm_args->nlm_cookie, nlm_args->status)

Ok, above are all operations of lockd server. This server corresponds with nfs server. Although lockd server will send rpc call to lockd client, but there are lock underlying associated with file. and here I don't why there may be more than 1 server? From point of lockd server, there are lots of client. But what's the matter from the point of lockd client? and lockd servers and lockd clients are put together? 



