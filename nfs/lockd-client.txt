再总结lockd client的实现, 这部分看起来简单一些,似乎lockd本来就很简单. 客户端做的事情就是接受用户的锁请求,现在本地nfs_inode上加锁，然后发送请求接受结果.

总算能理清一点思路，lockd分两个版本lockd3/lockd4,每个都有xdr和proc, 而这里都包括clnt和srv, statd协议应该在xnfs中规定，在google的缓存中找到了xnfs的实现,这些都是淘汰的东西,不用花太多时间。
(fs/lockd/mon.c)
这里实现statd的client功能, 那上面nsm相关的可能都是这里使用的. nsm就是监视系统是否重启,在其他系统中使用心跳机制,维护系统的状态,而这里只有在系统重启之后太通知另一方.
1. nsm_addr(nsm_handle)
    这是获取什么地址？  nsm_handle->sm_addr

2. nsm_create()
    创建rpc_clnt, 创建rpc_create_args，调用rpc_create. 为请求本地的statd服务做准备.
    -> rpc_reate(rpc_create_args)

介绍nsm使用的参数和结果
nsm_args
    * prog / vers / proc
    * mon_name
nsm_res
    * status / state
的确比较简单, 但需要弄清楚mon_name是什么东西？

3. nsm_mon_unmon(nsm_handle, proc, nsm_res)
    创建nsm_args,根据nsm_handle和proc，然后创建rpc_message,使用rpc调用. 使用nlm_handle组装nsm_args，这里主要使用nsm_handle->sm_priv/sm_mon_name，应该是监视的机器名称hostname或地址.
    -> nsm_create
    -> rpc_call_sync
    -> rpc_shutdown_client

4. nsm_monitor(nlm_host)
    监视这个host,把nlm_host->nsm_handle->sm_mon_name给用户层rpc.statd的database，就是一个文件. nsm_handle->sm_mon_name可能是nsm_handle->sm_name或者nsm_handle->sm_addrbuf
    -> nsm_mon_unmon(nsm_handle, NSMPROC_MON, rec) 这个是向本地发送的, and tell nsm how to callback nlm, and tell nlm that some host is crashed. see nsm_args->mon_name.

5. nsm_unmonitor(nlm_host)
    这里把rpc调用封装到注销监控一个host的过程中. 需要检查nsm_handle->sm_count =1, nsm_handle->sm_monitored = 1. 这个和上面的函数都是调用相同的rpc请求函数,使用的PROC不一样.
    -> nsm_mon_unmon(nsm_handle, NSMPROC_UNMON, res)

6. nsm_lookup_hostname(hostname, len)
    在nsm_handles队列中，找nsm_handle，它的sm_name和参数相通

7. nsm_lookup_addr(sockaddr)
    根据sockaddr查找nsm_handle

8. nsm_lookup_priv(nsm_private)
    根据nsm_private，他就是一个字符串

下面是创建nsm_handle
9. nsm_init_private
唉！ 把自己害苦了，这些东西根本不重要，xnfs的文档还找不到.
    创建nsm_handle->sm_priv.data,唯一的识别某个nlm_host,这个cookie会传给rpc.statd. cookie包括当前时间和nsm指针.
    -> ktime_get(timespec)
    -> timespec_to_ns

10. nsm_create_handle(sockaddr, salen, hostname, hostname_len)
    创建一个nsm_handle, nsm_handle里面有好多name,还有sockaddr, 这些都是谁的地址？应该是链接另一端的地址，都是需要监视的机器的地址，不过有多种表示方式. hostname/sockaddr
    nsm_handle->sm_addr = sockaddr
    nsm_handle->sm_addrbuf 是 nsm_handle->sm_addr的统一表示形式
    nsm_handle->sm_name = hostname
    -> nsm_init_private(nsm_handle)

11. nsm_get_handle(sockaddr, salen, hostname, hostname_len)
    查找一个nsm_handle,如果找到返回，如果找不到，创建一个. 在nlm_alloc_host中使用,创建nlm_host时，同时创建对应的nlm_handle.
    -> nsm_lookup_addr(sockaddr) 根据地址查找
    -> nsm_lookup_hostname(hostname) 根据hostname查找
    -> nsm_create_handle(sockaddr, slen, hostname, len)

12. nsm_reboot_lookup(nlm_reboot)
    根据nlm_reboot->priv查找nsm_handle, 通知这个nsm_handle表示的系统reboot,只是递增nsm_handle->sm_count.

13. nsm_release(nsm_handle)
    从全局队列nsm_handles中释放nsm_handle的关系，释放nsm_handle使用的内存

14. encode_nsm_string(xdr_stream, string)  / encode_mon_name

15. 构造rpc_procinfo nsm_procedures,只有两个NSMPROC_MON和NSMPROC_UNMON. 这里属于客户端操作，rpc_procinfo显然是client使用的信息,这里可能和server端混淆
    rpc_program nsm_program / nsm_version1
这里用来实现nsm的client工作，发送请求,但nsm_reboot_lookup应该是nsm请求使用的函数,因为nlm也接受nsm的rpc request.
但说nsm_reboot_lookup果然是nlm接受sm_notify请求使用的函数,它收到nlm_reboot信息,然后根据信息找到nsm_handle, 再遍历nlm_host, 如果它是client(nlm_server_hosts), 释放它请求的锁资源,然后处理它的reclaim请求.如果是server(nlm_client_hosts), 启动内核线程，重新请求它使用的锁.

client side NLM implement, lock handle
On server side, file could be opened, and lock would be added to file, so file state and lock state and client state would be managed, and some operation is async, and some operation would be needed by RPC, all these stata should be used. On client side, maybe only lock state is needed, and lock owner(process) .
(fs/lockd/clntlock.c)
1. data struct
    nlm_wait
    * list_head b_list
    * wait_queue_head_t b_wait   who will wait on here
    * nlm_host  yes, both lockd server and client side use it to denote host.
    * file_lock b_lock  lock info, what's the owner, pid?
    * b_reclaim / b_status  get to reclaim or grant callback status

client也有一个nlm_blocked, 不过他是nlm_wait队列.

   nlmsvc_binding nlmsvc_ops
    include two function, but hard to understand who client use it?  这是lockd server使用的,用来打开和关闭文件，但这两个文件是nfsd提供的. 真不想看了...
    fopen(svc_rqst, nfs_fh, file)
    fclose(file)

   nlmclnt_initdata  It is said this is similar to nfs_client_initdata? not good
    * hostname
    * sockaddr address / addrlen
    * protocol
    * nfs_version
    * noresport

2. operation 这里不好说在做什么,包括nlm_host如何管理client添加的锁,等待的锁nlm_wait.
这里主要创建了nlm_host,但是nfs_server在管理它,这样在使用锁时，把file_lock给这个nlm_host.这里还有server grant处理，server重启后的reclaim处理.
    a. nlmclnt_init(nlmclnt_initdata)
        这个在nfs的模块加载函数中使用.
        -> lockd_up 它竟然启动lockd的server线程
        -> nlmclnt_lookup_host(nlmclnt_initdata->sockaddr, len, nlmclnt_initdata->protocol, nlm_version, nlmclnt_initdata->hostname, nlmclnt_initdata->noresvport) It is said here is to setup per-NFS mount point lockd data structures, so host should be NFS server.  But nlm_host->server=0, so what's the hell it?  这里是lockd client,所以nlm_host是远程nfs server系统.但nlm_host->server=0.
        if nfs_version = 2, nlm_version=1
        else nlm_version = 4

    b. nlmclnt_done(nlm_host)
        -> nlmclnt_release_host(nlm_host)
        -> lockd_down

    c. nlmclnt_prepare_block(nlm_host, file_lock)
        构造一个nlm_wait. queue up a blocked lock , so GRANTED request can see it. nlm_wait refer to nlm_host and file_lock, beside initial its wait queue and add it to nlm_blocked, which contain all blocked lock.

    d. nlmclnt_finish_block(nlm_wait)
        release it from nlm_blockd, and free it.

    e. nlmclnt_block(nlm_wait, nlm_rqst, timeout)
        why does nlm_rqst appear? it just need lock result? This function implement wait process. 让当前任务在nlm_wait->b_wait上面等待. 
        -> wait_event_interruptible_timeout(nlm_wait->b_wait, nlm_wait->b_status = nlm_lck_blocked, timeout)

    f. nlmclnt_grant(sockaddr, nlm_lock)
        这是lockd server实现的东西. 这里的server不尽是处理锁请求，因为server也会给client发送消息,给它授权锁. GRANTED call is received, and some blocked lock request should be unblocked. For local file lock, file_lock is added to inode->i_flock, but here, all blocked lock is added to nlm's nlm_blocked list. Traverse nlm_blocked, check all nlm_wait's file_lock, compare such items:
        *  nlm_wait->file_lock-> start/end = nlm_lock->file_lock->**
        * nlm_wait->file_lock->fl_u->nfs_fl.owner->nlm_lockowner->pid = nlm_lock->svid 哪里填充的file_lock->fl_u->nfs_fl.
        * nlm_wait->nlm_host->h_addr = sockaddr
        * NFS_FH(nlm_wait->file_lock->file->f_path.dentry->d_inode->nfs_inode->nfs_fh = nlm_lock->nfs_fh
        -> nlm_wait->status = nlm_granted  update nlm_wait's state and wake up blocked process
        -> wait_up(nlm_wait->b_wait)

    g. nlmclnt_recovery(nlm_host)
        这个是收到sm_notify使用的函数，通知client重新申请它所使用的锁. This procedure deal with recovery of locks after server crsh, reclaim all locks. spawn a separate reclaimer ktrhead
        -> kthread_run(reclaimer, ...)

    h. reclaimer(nlm_host)
        上面的任务使用的内核函数.这里需要重新确认已经添加的锁,所以所有的锁必须都记录起来,nlm_host->h_grantet队列管理所有的file_lock->fl_u->nfs_fl->list. 所有把nlm_host->h_granted给nlm_host->h_reclaim, reclaim all file_lock, if it make it, file_lock is restored to nlm_host->h_granted. 
        -> nlmclnt_reclaim(nlm_host, file_lock)  这是client的一个rpc request实现.
        and here also deal with nlm_blocked's nlm_wait, because nlm_host reboot, so all blocked lock related to the nlm_host should be released ,and set b_status to nlm_lck_denied_grace_period, and wake up its process.
        -> wake_up(nlm_wait->b_wait)
        -> nlmclnt_release_host(nlm_host)

client side NLM implementation: RPC procedure
NLM client also recevie rpc call? now, send rpc call should be easy, just create rpc_rqst, and fill with parameter.. nlm client的确会接受rpc request,但那些处理都放在LOCKD server中一块了，只不过如何相应请求看底层的管理. server的话就是锁文件,而client就是把锁结果告诉应用者.


这里是客户端管理锁的实现
    nlm_lockowner, it just pack nlm_lock and pid/fl_owner_t. 恩,这是一个纯client使用的东西., 它会关联多个file_lock??  哪里把它填充到file_lock中的
        * list_head list 它在nlm_host->h_lockowners队列中
        * count
        * nlm_host *host
        * fl_owner_t owner  it is files_struct??
        * pid
    nfs_lock_info 这个是嵌在file_lock
        * state
        * nlm_lockwoner
        * list_head     这个链表是nlm_host->h_granted队列.
        
管理nlm_lockowner
1. nlm_cookie: this should identify all lock requested by client.
    nlmclnt_next_cookie 递增nlm_cookie->data

2. nlm_get_lockowner(nlm_lockowner)
    increase nlm_lockowner->count

3. nlm_put_lockowner(nlm_lockowner)
    decrease(nlm_lockowner->count), if count is 0, destroy nlm_lockowner, free it.
    -> nlmclnt_release_host(nlm_lockowner->nlm_host)

3. nlm_pidbusy(nlm_host, pid)
    traverse nlm_host->h_lockowners, find check if there is a nlm_lockowner whose pid is equal to given pid.

4. __nlm_alloc_pid(nlm_host)
    alloc a unused pid from nlm_host, this pid is not task_struct's pid, and it's unique for lock requestor, from the point of nlm_host.
    pid = nlm_host->h_pidcount

5. __nlm_find_lockowner(nlm_host, fl_owner_t)
    find a nlm_lockowner from nlm_host->h_lockowners, its fl_owner_t is equal to given parameter. and increase its count if fount.

6. nlm_find_lockowner(nlm_host, fl_owner_t)
    try to locate a nlm_lockowner with nlm_host & fl_owner_t, if none, create a new one, add it to nlm_host->h_lockowners.在nlmclnt_locks_init_private中使用
    -> __nlm_find_lockowner
    -> __nlm_alloc_pid
    -> nlm_get_host
    There is a operation model used often, when a new object is to be located or created, and some lock is used to pretect the container.
    1. lock to pretect container, and try to find, and unlock, if a object is found, return it.
    2. create a new object, notice this could make process slept.
    3. lock to pretect container again, and first try to find, if an object is found, return it; if none is found, insert the new one into container, and unlock.
    4. at last,  free the object just created if it is not used.

封装和释放nlm_rqst
7. nlmclnt_setlockargs(nlm_rqst, file_lock)
    Oh, initialize nlm_args for TEST/LOCK/UNLOCK/CANCEL calls, wawawa, so facilitiy. Here is to initialize nlm_rqst->nlm_args->nlm_lock. see the detail:
    -> nlmclnt_next(nlm_args->cookie)
    -> nlm_lock->nfs_fh = file_lock->file==>nfs_inode->nfs_fh
    -> nlm_lock->caller  hostname
    -> nlm_lock->xdr_netobj->data = nlm_rqst->a_owner, it is string composed by file_lock's pid and hostname.  所谓的xdr_netobj
    -> nlm_lock->file_lock-> {start, end, type}

8. nlmclnt_release_lockargs(nlm_rqst)

这是nfs中使用的东西
9. nlmclnt_proc(nlm_host, cmd, file_lock)
    Perform a signal client-side lock request, mainly start rpc call. file_lock should be passed from vfs interface, or this function is called by nfs client. 这个函数在nfs中使用.
    -> nlm_get_host(nlm_host)
    -> nlm_alloc_call(nlm_host)
    -> nlmclnt_locks_init_private(file_lock, nlm_host) 初始化file_lock->fl_u->nfs_lock_info
    -> nlmclnt_setlockargs(nlm_host, file_lock)
    -> nlmclnt_lock(nlm_rqst, file_lock)  SETLKW & F_LCK
    -> nlmclnt_unlock(nlm_rqst, file_lock)  SETLKW & F_UNLCK
    -> nlmclnt_test(nlm_rqst, file_lock)    GETLK
    -> file_lock->fl_ops->fl_release_private(file_lock)  函数是同步实现的?为何直接释放?

下面是层层封装rpc call, 还有SUNRPC使用的回调函数
10. nlm_alloc_call(nlm_host)
    alloc nlm_rqst, and before rpc call, we don't care its variety about rpc, and here just initialize its file_lock
    -> locks_init_lock(nlm_rqst->nlm_args->nlm_lock->file_lock)
    -> locks_init_lock(nlm_rqst->nlm_res->nlm_lock->file_lock)
    -> schedule_timeout_interruptible(5*HZ) if memory is in need, wait for 5s.

11. nlmclnt_release_call(nlm_rqst)
    release nlm_rqst, does this used by many op or nlm object? or reused? decrease nlm_rqst->a_count, destroy it when nlm_rqst->a_count=0,释放内存. 这个和rpc request使用.传递rpc_message信息.
    -> nlmclnt_release_host(nlm_rqst->nlm_host)
    -> nlmclnt_release_lockargs(nlm_rqst->nlm_args)

12. nlmclnt_rpc_release(data)
    this should be rpc callback
    -> nlmclnt_release_call(nlm_rqst)

13. nlm_wait_on_grace(wait_queue_head_t)
        Here we just wait on the queue for some time, if some event(signal) wait process, return EINTR.

14. nlmclnt_call(rpc_cred, nlm_rqst, proc)
        这是为下面的rpc request做准备. nlm_rqst has been prepared, and rpc_message is kind of simple, only contain args,res,rpc_procinfo,rpc_cred. Besides start rpc call, here focus grace-time deal.
        -> nlm_bind_host(nlm_host)  return rpc_clnt
        -> rpc_call_sync(rpc_clnt, rpc_message, 0)
        -> wake_up_all(nlm_host->h_gracewait)  wake up ops which run before and waited on such host.
        -> nlm_wait_on_grace(nlm_host->h_gracewait) wait on nlm_host->h_gracewait for some time, or until above sentense is executed.
        这里使用同步RPC调用,错误处理还是比较简单, 主要是grace_period

15. __nlm_async_call(nlm_rqst, proc, rpc_message, rpc_call_ops)
    async NLM rpc call, not ever seen before. why does here use rpc_task? It should be too lower-class to be used generic user.  Setup rpc_task_setup
    -> nlm_bind_host(nlm_host)  重新创建rpc_clnt, 这个如果一段时间不用,就rebind
    -> rpc_run_task(rpc_task_setup)

16. nlm_do_async_call(nlm_rqst, proc, rpc_message, rpc_call_ops)
    -> __nlm_async_call(nlm_rqst, proc, rpc_messge, rpc_call_ops)
    -> rpc_put_task(rpc_task)  If above call failed.

17. nlm_async_reply(nlm_rqst, proc, rpc_call_ops)
    Why do here reply? reply what?  client should just call rpc and wait for reply.
    -> nlm_do_async_call(nlm_rqst, proc, rpc_message, rpc_call_ops)

18. nlmclnt_async_call(rpc_cred, nlm_rqst, proc, rpc_call_ops)
    Although the calls are asyncronous, we still attempt to wait for completion , in order to guarante to complete and track the lock state.
    -> __nlm_async_call(nlm_rqst, proc, rpc_message, rpc_call_ops)
    -> rpc_wait_for_complete_task(rpc_task)
    -> rpc_put_task

19. nlmclnt_test(nlm_rqst, file_lock)
    TEST procedure, rpc call is simple, error handling is needed. When return nlm_granted, file_lock->fl_type should be modified F_UNLCK, this is subtle. when return nlm_lck_denied, copy nlm_rqst->nlm_res->nlm_lock->file_lock to file_lock.
    -> nlmclnt_call(rpc_cred, nlm_rqst, NLMPROC_TEST)

20. nlmclnt_locks_copy_lock(file_lock, file_lock)
    This is lock callback function, used to copy locks, and modify private data about nfs lockowner.
    -> nlm_get_lockowner(file_lock->fl_u->nfs_lock_info->nlm_lockowner) and here are something about nfs_lock_info, insert this file_lock to nlm_lock->h_granted list.

21. nlmclnt_locks_release_private(file_lock)
    delete file_lock->fl_u->nfs_lock_info from nlm_lock->h_granted list, and release nlm_lockowner
    -> nlm_put_lockowner(file_lock->fl_u->nfs_lock_info->nlm_lockowner
    Yes, file_lock_manager nlmclnt_lock_ops. some similar to cpp's class copy.  不知道自己在说什么！！  这是是file_lock的回调函数,在锁释放/拷贝时使用. 但为何没有lock_manager_operations操作? 如果自己申请阻塞,当另外线程把它释放了呢?

22. nlmclnt_locks_init_private(file_lock, nlm_host)
    initial nfs_lock_info's state, nlm_lockowner and list, and file_lock->fl_ops=file_lock_manager

23. do_vfs_lock(file_lock)
    This is hard to understand, and here handle posix lock and flock, and revoke vfs functions.Yes, here just lock a 'file' as lock file system, and won't revoke something like f_ops->lock. File lock is function provided by OS, and has nothing to do with local file system, because lock just works not below inode, or IOW, lock is managed in inode, file(no dentry). But for NFS, we have to deal with data consistency, so lock should work over network, beside 'local lock', server lock should be implemented. when server lock fails, local lock should be modified according to error handling.
    -> posix_lock_file_wait(file_lock->file, file_lock)
    -> flock_lock_file_wait(file_lock->file, file_lock)  Does here need file lock?

24. nlmclnt_lock(nlm_rqst, file_lock)
    -> nsm_monitor(nlm_host) make sure server is monitored
    First we lock locally, and if fail, just return.
    -> do_vfs_lock(file_lock)
    -> nlmclnt_prepare_block(nlm_rqst->nlm_host, file_lock) So each lock requst costs a nlm_wait. rpc_wait link nlm_host, file_lock, use wait_queue_head_t.
    -> nlmclnt_call(rpc_cred, nlm_rqst, NLMPROC_LOCK)
    -> nlmclnt_block(nlm_wait, nlm_rqst, timeout) If above call return nlm_lck_blocked, here we just wait for some time. and will loop to call last 2 functions.
    -> nlmclnt_cancel(nlm_host, nlm_rqst->nlm_args->block, file_lock) In server, lock could be blocked, and indicated by nlm_res->status(rpc result),  we should retry, until we get error or get lock, if we get blocked, and request should not be blocked, cancel the lock, and why does it not release local lock?
    -> If lock is granted, compare nfs_lock_info->state with nlm_host->state, if they are different, host reboot, we should relock. do lock lock inode again? why? network fail?
    -> nlmclnt_finish_block(nlm_wait) if we cancel server lock or get lock, destroy nlm_wait.
    -> nlmclnt_release_call(nlm_rqst)
    -> do_vfs_lock
    -> nlmclnt_async_call(rpc_cred, nlm_rqst, NLMPROC_UNLOCK ...) If we get error, unlock locally and unlock server's lock.

25. nlmclnt_reclaim(nlm_host, file_lock)
    Send LOCK call again
    -> nlmclnt_setlockargs(rpc_rqst, file_lock)
    -> nlmclnt_call(rpc_cred, nlm_rqst, NLMPROC_LOCK)

26. nlmclnt_unlock(nlm_rqst, file_lock)
    UNLOCK procedure, unlock locallay, and sent UNLOCK call.
    -> do_vfs_lock(file_lock)
    -> nlmclnt_async_call(rpc_cred, nlm_rqst, UNLOCK ...)
    -> nlmclnt_release_call(nlm_rqst)

27. nlmclnt_unlock_callback(rpc_task, data)
    where is it called? here may handle rpc_task error, rebind to setup link and restart rpccall..
    -> nlm_rebind_host(nlm_rqst->nlm_host)
    -> rpc_restart_call(rpc_task)
    
    rpc_call_ops nlmclnt_unlock_ops,UNLOCK使用的回调函数,释放nlm_wait数据
        rpc_call_done = nlmclnt_unlock_callback
        rpc_release = nlmclnt_rpc_release 释放锁

28. nlmclnt_cancel(nlm_host, file_lock)
    cancel blocked lock request
    -> nlm_alloc_call(nlm_host)
    -> nlmclnt_setlockargs(nlm_rqst, file_lock)
    -> nlmclnt_async_call(rpc_cred, nlm_rqst, NLMPROC_CANCEL, nlmclnt_cancel_ops)

29. nlmclnt_cancel_callback(rpc_task, data)
    这是lock的rpc请求使用的回调函数,callback function again. there are correspond callback ops for different asynchronic rpc call. Here mainly handle errors. 如果rpc请求没有成功,重新发送请求.
    -> nlm_rebind_host(nlm_rqst->nlm_host)
    -> rpc_restart_call(rpc_task)

    rpc_call_ops nlmclnt_cancel_ops CANCEL请求使用的回调函数
        rpc_call_done = nlmclnt_cancel_callback
        rpc_release = nlmclnt_rpc_release  释放nlm_rqst数据,它的确是一个client使用的类型,即使在nlmsvc中也涉及,也是为了发送GRANT使用了. 所以并没有全局管理这个类型的变量，在每个rpc请求中使用回调函数释放它.

后面nlmclnt_lock/unlock/test是处理请求的函数,他们包装nlmclnt_*_call,  nlmclnt_proc使用nlmclnt_lock之类的函数, 而nlmclnt_proc是file_operation->lock函数. nlmclnt_cancel/test只处理锁的rpc请求，而nlmclnt_lock/unlock还要处理vfs的锁.
sys_lock发送请求后,把nfs_server->nlm_host传过来,然后创建nlm_rqst, 包装nlm_host/nlm_args/nlm_res;同时包装file_lock,构造(nlm_args的)nlm_lock, 然后发送rpc requst.发送完成后处理file_lock, 自动释放nlm_rqst.

   看完了，可恶的总结 

    
    

    
    
    




