
(net/sunrpc/svc.c)
1. 数据结构
    svc_serv
    * svc_program
    * svc_stat
    * sv_lock
    * sv_nrthreads
    * sv_maxconn  最大支持的链接数,0表示与线程数成正比
    * sv_max_payload
    * sv_max_mesg
    * sv_xdrsize

    * sv_persocks
    * sv_tempsocks
    * sv_tmpcn
    * timer_list sv_temptimer

    * sv_name
    * sv_nrpools
    * sv_pools
    * sv_shutdown(svc_serv, net)

    * sv_thread_fn
    * list_head sv_cb_list sv_cb_waitq
    * svc_xprt  sv_bc_xprt
又是好大的数据结构
    svc_pool
    * sp_id     pool id
    * sp_lock   
    * list_head sp_threads
    * sp_sockets
    * sp_nrthreads  #idle server threads
    * sp_sockets    #pending socket
    * sp_all_threads    # pool的threads数量
    * svc_pool_stats

    svc_rqst
    * rq_list       # idle list
    * rq_all        # all threads
    * svc_xprt
    * sockaddr_storage  rq_addr rqlen       # peer address
    * sockaddr_storage  rq_daddr rq_daddrlen    #这是什么地址?
    * svc_serv  rq_server
    * svc_pool  rq_pool
    * svc_procedure rq_procinfo
    * auth_ops  rq_authop
    * rq_flavor
    * svc_cred
    * rq_xprt_ctxt
    * svc_deferred_req
    * rq_usedeferral
    * rq_xprt_hlen
    * xdr_buf rq_arg, rq_res
    * page rq_pages[RPCSVC_MAXPAGES]
    * page rq_respages
    * rq_resused
    * kvec rq_vec[RPCSVC_MAXPAGES]

    * rq_xid
    * rq_prog, rq_vers, rq_proc, rq_prot 为何这里有protocol?
    * rq_secure
    * rq_argp, rq_resp, rq_auth_data
    * rq_reserved
    * cache_req rq_chandle 只有在service端才使用cache?
    * rq_dropme
    * auth_domain rq_client,rq_gssclient
    * rq_cachetype
    * svc_cacherep  rq_cacherep
    * rq_splice_ok
    * wait_queue_head_t rq_wait
    * task_struct   rq_task 每个svc_rqst都对应一个task_struct??

    svc_deferred_req
    * prot  
    * svc_xprt xprt
    * sockaddr_storage  addr / addrlen
    * sockaddr_storage  daddr / daddrlen
    * cache_deferred_req    handle
    * xprt_hlen
    * argslen
    * args[0]   难道这里还要和用户层交互? 如果服务真的在用户曾怎么办?

    svc_program
    * svc_program next
    * pg_prog
    * pg_lovers, pg_hivers, pg_nvers
    * svc_version   pg_vers
    * pg_name
    * pg_class  #class name 认证相关?
    * svc_stat  pg_stats
    * pg_authenticate(svc_rqst)

    svc_version
    * vs_vers, vs_nproc
    * svc_procedure vs_proc
    * vs_xdrsize
    * vs_hidden 不要向rpcbind注册,只有nfsacl使用
    * vs_dispatch(svc_rqst, be32)

    svc_procedure
    * svc_procfunc(svc_rqst, argp, resp)
    * pc_decode, cp_encode, pc_release
    * pc_argsize, pc_ressize, pc_count
    * pc_cachetype, pc_xdrreszie


    svc_pool_map
    * count, mode   POOL_GLOBAL, POOL_PERCPU, POOL_PERNODE
    * npool 
    * pool_to   maps pool id to cpu or node
    * to_pool   maps cpu or node to pool id
可以设置pool的模式，系统自己选择是如果系统有多个node,使用POOL_PERNODE,如果系统有2个以上cpu，使用POOL_PERCPU, 否则POOL_GLOBAL.这个数据结构的作用是把node与pool互相映射，node表示online cpu/node的索引, pool索引是从1开始的计数. 这个数据结构是在一个svc_pool_map全局变量使用.

关于svc_pool_map的操作
    1. 创建销毁等
    param_set_pool_mode 设置svc_pool_map
    param_get_pool_mode 获取svc_pool_map
    svc_pool_map_choose_mode  系统自动选择pool mode
    svc_pool_map_alloc_arrays 创建svc_pool_map的to_pool和pool_to
    svc_pool_map_init_percpu(svc_pool_map)  根据pool mode初始化to_pool和pool_to数组
    svc_pool_map_init_pernode(svc_pool_map) 和上面一样
    svc_pool_map_get()  如果svc_pool_map->count=0,初始化svc_pool_map, 综合上面的实现
    svc_pool_map_put()  递减svc_pool_map->count,如果减到0, 销毁svc_pool_map,他是全局变量，只销毁to_pool和pool_to
    2. svc_pool_map_set_cpumask(task_struct, pidx)
        把task_struct的cpu mask设为pidx对应的,让它只能在某些cpu上执行
        -> set_cpus_allowed_ptr(task_struct, mask)
    3. svc_pool_for_cpu(svc_serv, cpu)
        找到cpu索引对应的pool索引, 然后返回svc_serv->sv_pools[pidx%sv_npools], svc_serv->sv_npools决定sv_pools的数据。

下面是svc_serv的操作
    1. svc_rpcb_setup(svc_serv, net)
        创建svc_serv之前，删除rpcbind中已经创建的server.
        -> rpcb_create_local(net)
        -> svc_unregister(svc_ser, net)

    2. svc_rpcb_cleanup(svc_serv, net)
        -> svc_unregister(svc_serv, net)
        -> rpcb_put_local(net)

    3. svc_uses_rpcbind(svc_serv)
        判断rpc_program是否对rpcbind不可见 hide,看svc_version->vs_hidden. 遍历svc_serv->svc_program中的所有svc_version, 只要有一个可见，整个rpc_program都可见.

    4. __svc_create(svc_program, bufsize, npools, shutdown )      
        创建svc_serv和svc_pool使用的内存, 对svc_serv的初始化包括
        * sv_name = svc_program->pg_name
        * sv_program = svc_program
        * sv_nrthreads = 1
        * sv_stats = svc_program->svc_stats
        * sv_max_payload = bufsize  如果这个数为0，使用4096 这个值限制什么?
        * sv_max_mesg = sv_max_payload, 但它是对PAGE_SIZE向上对其的
        * sv_shutdown = shutdown 回调函数,如果参数无效使用svc_rpcb_cleanup
        * sv_xdrsize 为何参数svc_program会关联多个? 遍历所有svc_program->svc_version,找svc_version->vs_xdrsize.
        * 其他链表之类的
        * sv_nrpools = npools
        * svc_pool, 创建npools个svc_pool, 对每个初始化
        -> svc_uses_rpcbind(svc_serv)
        -> svc_rpcb_setup(svc_serv, net) 这里面就是rpcb_unregister?

      svc_create(svc_program, bufsize, shutdown)
      对上面进行包装,默认npools是1

    5. svc_create_pooled(svc_program, bufsize, shutdown, svc_thread_fn, module)
        同样对上面进行包装，npools使用svc_pool_map制定的
        -> svc_pool_map_get()
        -> __svc_create(svc_program, bufsize, npool, shutdown)
        然后制定svc_serv->sv_function和svc_serv->sv_module

    6. svc_destory(svc_serv)
        -> svc_sock_update_bufs(svc_serv)           -> svc_serv->sv_nrthreads不为0
        -> del_timer_sync(svc_serv->sv_temptimer)   
        -> svc_close_all(svc_serv)      关闭svc_xprt
        -> svc_serv->sv_shutdown(svc_serv, net) 
        -> cache_clean_deferred(svc_serv)       为何这里使用cache?
        -> svc_serv_is_pooled()     只要sv_function有效?
        -> svc_pool_map_put     释放svc_pool_map
        释放svc_serv->sv_pools和svc_serv.

    7. svc_init_buffer(svc_rqst, size, node)
        -> svc_is_backchannel(rpc_rqst) back channel 使用fore channel buffers
        分配size需要的page,把这些page放到svc_rqst->rq_pages
        -> alloc_pages_node(node, GFP_KERNEL, 0)

    8. svc_release_buffer(svc_rqst)
        释放svc_rqst->rq_pages中的所有page

    9. svc_prepare_thread(svc_serv, svc_pool, node)
        创建rpc_rqst, 一个rpc_rqst对应一个thread? 递增svc_serv->sv_nrthreads和svc_serv->sv_nrthreads, 给svc_rqst->rq_argp和rq_resp分配内存,大小为svc_serv->sv_xdrsize.
        -> init_waitqueue_head(svc_rqst->rq_wait)
        -> svc_init_buffer(svc_rqst, svc_serv->sv_max_mesg, node)

    10. choose_pool(svc_serv, svc_pool, state)
        如果svc_pool有效,返回它,否则根据state选择svc_serv->sv_pools中的一个

    11. choose_victim(svc_serv, svc_pool, state)
        找一个thread杀掉, 先选一个pool, 它的svc_pool->sp_all_threads不为空, 这个队列是svc_rqst队列,把队列上的第一个rpc_rqst取出来,把它从队列中释放,返回svc_rqst->rq_task.

    12. svc_set_num_threads(svc_serv, svc_pool, nrservs)
        svc_serv创建或结束线程,使svc_serv->sv_nrthreads=nrservs，如果svc_pool有效，尽对它进行操作,否则均匀的添加或删除所有svc_pool的进程.
        如果要创建线程
        -> choose_pool(svc_serv, svc_pool, state)  如果svc_pool有意义，就用它，否则使用state对应的.
        -> svc_pool_map_get_node(svc_pool->sp_id)
        -> svc_prepare_thread(svc_serv, svc_pool, node)  node主要用来分配内存
        -> kthread_create_on_node(svc_serv->sv_function, svc_rqst, node, svc_serv->sv_name)
        -> svc_pool_map_set_cpumask(svc_rqst->rq_task, svc_pool->sp_id)
        -> svc_sock_update_bufs(svc_serv)
        -> wake_up_process(task_struct)
        如果要删除线程
        -> choose_victim(svc_serv, svc_pool, state)
        -> send_sig(SIGINT, task_struct, 1)

    13. svc_exit_thread(svc_rqst)
        释放svc_rqst->rq_pages, rq_argp, rq_resp, rq_auth_data. 把它从svc_pool->sp_all_threads中释放, 释放它自己
        -> svc_destroy(svc_serv)

    14. __svc_rpcb_register4(net, program, version, protocol, port)
        先创建一个sockaddr,使用INADDR_ANY+port
        -> rpcb_v4_register(net, program, version, sockaddr, netid)
        -> rpcb_register(net, program, version, protocol, port) 如果rpcv4不支持

    15. __svc_rpcb_register6(net, program, version, protocol, port)
        rpcb_v4_register(net, program, version, sockaddr_in6, netid6)  netid6是字符串

    16. __svc_register(net, progname, program, version, family, protocol, port)
        封装上面两个函数,为何这里注册没有svc_program的任何事情?

    17. svc_register(svc_serv, net, family, proto, port)
        向local的rpcbind注册svc_serv,还有远程的rpcbind? 这里svc_serv->svc_program中可能有多个svc_program. 遍历所有的rpc_version
        -> __svc_register(net, progname, program, version, i, family, protocol, port)

    18. __svc_unregister(net, program, version, progname)
        通过注册一个空的svc_program,取代之前的服务
        -> rpcb_v4_register(net, program, version, NULL, 0)

    19. svc_unregister(svc_serv, net)
        对svc_serv->svc_program中所有svc_version，调用
        -> __svc_unregister(net, program, version, ...)

    20. svc_process_common(svc_rqst, kvec argv, kvec resv)
        这是rpc请求的共用处理过程, 把结果给resv
        -> svc_rqst->svc_xprt->xpt_ops->xpo_prep_reply_hdr(svc_rqst)
        填充resv中的rpc包头
        * xid <= svc_rqst->rq_xid
        * REPLY 1  calldir
        * status  => 把位置记录在一个变量中,默认是0 ACCEPT
        * prog, vers, proc <= 从argv中获取
        * RPC_SUCCESS  * 结果?
        从svc_rqst->svc_serv->sv_program中取出与prog对应的svc_program
        -> svc_authenticate(svc_rqst, auth_stat)
        -> svc_program->pg_authenticate(svc_rqst)
        svc_rqst->rq_procinfo = svc_serv->svc_program->svc_version->svc_procedure
        初始化svc_rqst->rq_argp,rq_resp两块内存
        -> svc_reserve_auth(svc_rqst, svc_procedure->pc_xdrressize<<2)
        如果svc_procedure->vs_dispatch有效,调用它，否则调用下面过程
        * svc_procedure->pc_decode(svc_rqst, argv->iov_base, svc_rqst->rq_argp)
        * svc_procedure->pc_func(svc_rqst, svc_rqst->rq_argp, svc_rqst->rq_resp)
        * svc_procedure->pc_encode(svc_rqst, resv->iov_base_len, svc_rqst->rq_resp)
        * svc_procedure->pc_release(svc_rqst, NULL, svc_rqst->rq_resp)
        * svc_authorise(svc_rqst) 验证通过，释放验证资源之类的
        如果过程中出现错误,同样会组好包，放到resv中。
        这里没有发送功能，而且服务的完成分成两种，一种是间接dispatch,一种直接调用

    21. svc_process(svc_rqst)
        这里的主要功能是为了svc_process_common准备好resv和argv. 上面在decode和encode时使用中转空间是rq_argp, rq_resp. 这里argv就是svc_rqst->rq_arg.head[0], 但svc_rqst->rq_res还不能用，使用svc_rqst->rq_respages初始化它,第一页给svc_rqst->rq_res.head[0], tail[0]为空, rq_res.pages使用rq_respages的第二页
        -> svc_process_common(svc_rqst, argv, resv)
        -> svc_send(svc_rqst)

    22. bc_svc_process(svc_serv, rpc_rqst, svc_rqst)
        和上面类似，不过多了一个rpc_rqst参数,而且最后把结果通过bc发出去,关联rpc_rqst->rpc_xprt = svc_serv->sv_bc_xprt, rpc_rqst->rq_server = svc_serv. 把rpc_rqst->rq_rcv_buf/rq_snd_buf给svc_rqst->rq_arg/rq_res，这些都是xdr_buf,
        -> svc_process_common(svc_rqst, argv, resv) 这些都是svc_rqst的, 把svc_rqst->rq_res给rpc_rqst->rq_snd_buf.
        -> bc_send(rpc_rqst)

(net/sunrpc/svc_xprt.c)
又是一个大头。。好像还不算多，比rpc_xprt少点吧
1. 数据结构
    svc_xprt_class
    * xcl_name      xcl_name
    * module        xcl_owner
    * svc_xprt_ops  xcl_ops
    * list_head xcl_list
    * xcl_max_payload  这个单位是什么?

    svc_xprt_ops
    * xpo_create(svc_serv, net, sockaddr, int, int) 地址啥用? 返回svc_xprt
    * xpo_accept(svc_xprt) 返回svc_xprt
    * xpo_has_wspace(svc_xprt)
    * xpo_recvfrom(svc_rqst)
    * xpo_prep_reply_hdr(svc_rqst)
    * xpo_sendto(svc_rqst)
    * xpo_relese_rqst(svc_rqst)
    * xpo_detach(svc_xprt)
    * xpo_free(svc_xprt)
    这里面有的参数svc_xprt,有的是svc_rqst，比较奇怪啊？

    svc_xprt
    * svc_xprt_class
    * svc_xprt_ops 重复了
    * kref xpt_ref 貌似很重要的东西
    * lsit_head xpt_list
    *           xpt_ready
    * flags
    * svc_resv xpt_server
    * xpt_reserved
    * xpt_auth_cache
    * xpt_deferred
    * sockaddr_storage  xpt_local/len
    * sockaddr_storage  xpt_remote/len
    * rpc_wait_queue    xpt_bc_pending 也有这个东西?
    * list_head xpt_users
    * net, rpc_xprt

    svc_xpt_user
    * list_head list
    * callback(svc_xpt_user)  嵌在其他数据结构里面，提供回调功能
    操作:
        * unregister_xpt_user
        * register_xpt_user 只有链表操作，但注册时保证svc_xprt->xpt_flags不是XPT_CLOSE

svc_xprt_class相关操作
    1. svc_reg_xprt_class(svc_xprt_class) / svc_unreg_xprt_class(svc_xprt_class)
        所有svc_xprt_class在svc_xprt_class_list中，使用xpl_name区分

svc_xprt操作
    2. svc_xprt_free(kref)
    -> svcauth_unix_info_release(svc_xprt)
    -> put_net
    -> xprt_put(svc_xprt->xpt_bc_xprt)
    -> svc_xprt->svc_xprt_ops->xpo_free(svc_xprt)

    3. svc_xprt_put(svc_xprt)
    -> kref_put(svc_xprt->kref, svc_xprt_free) kref方式的释放资源

    4. svc_xprt_init(net, svc_xprt_class, svc_xprt, svc_serv)
    初始化svc_xprt,包括 xpt_class, xpt_ops, kref, xpt_server, 各种list, xpt_bc_pending, net,还有把svc_xprt->xpt_flags设为XPT_BUSY

    5. __svc_xpo_create(svc_xprt_class, svc_serv, net, family, port, flags)
    偶，这里啥都没有，构造一个sockaddr,但使用INADDR_ANY
    -> svc_xprt_class->xcl_ops->xpo_create(svc_serv, net, sockaddr, len, flags)

    6. svc_create_xprt(svc_serv, xprt_name, net, family, port, flags)
    遍历svc_xprt_class_list,找一个svc_xprt_class, 封装上面的函数
    -> __svc_xpo_create(svc_xprt_class, svc_serv, net, family, port, flags)
    然后是一些关系，把svc_xprt添加到svc_serv->sv_permsocks队列中,这里返回端口号

    7. svc_xprt_copy_addrs(svc_rqst, svc_xprt)
    地址拷贝 svc_rqst->rq_addr = svc_xprt->xpt_remote, svc_rqst->rq_daddr = svc_xprt->xpt_local

    8. svc_thread_enqueue(svc_pool, svc_rqst) / svc_thread_dequeue(svc_pool, svc_rqst)
    把svc_rqst添加到svc_pool->sp_threads，看来一个svc_rqst对应一个thread, 浪费吧

    9. svc_xprt_has_something_to_do(svc_xprt)
    如果svc_xprt->xpt_flags包含XPT_CONN,XPT_CLOSE,返回true,否则如果包含XPT_DATA,XPT_DEFERRED
    -> svc_xpt->xpt_ops->xpo_has_wspace(svc_xprt)

    10. svc_xprt_enqueue(svc_xprt)
    -> svc_xprt_has_something_to_do(svc_xprt) 表示svc_xprt有空，还是没空? 
    如果svc_xprt->xpt_flags带有XPT_BUSY,则退出.
    根据当前cpu获取一个svc_pool, 从svc_pool->sp_threads中取出一个svc_rqst
    -> svc_thread_dequeue(svc_pool, svc_rqst) 然后把svc_rqst和rpc_xprt关联起来
    -> wake_up(svc_rqst->rq_wait) 唤醒svc_rqst等待队列上的任务?什么任务?
    如果svc_pool->sp_threads空了，把svc_xprt放到svc_pool->sp_sockets队列上，没有等待操作.

    11. svc_xprt_dequeue(svc_pool)
    从svc_pool->sp_sockets队列上取出一个svc_xprt，这是所有等待的svc_xprt

    12. svc_xprt_received(svc_xprt)
    去除svc_xprt->xpt_flags上的XPT_BUSY标志
    -> svc_xprt_enqueue(svc_xprt)

    13. svc_reserve(svc_rqst, space)
    每个request在svc_xprt上预留一段内存给reply使用. 如果svc_xprt->xpt_reserved大于space, 减小xpt_reserved.
    -> svc_xprt_enqueue(svc_xprt)

    14. svc_xprt_release(svc_rqst)
    -> svc_rqst->rq_xprt->xpt_ops->xpo_release_rqst(svc_rqst)
    释放svc_rqst->rq_deferred
    -> svc_free_res_pages(svc_rqst)释放svc_rqst->rq_respages
    -> svc_reserve(svc_rqst, 0) 这是什么意思？ 释放?
    -> svc_xprt_put(svc_xprt)

    15. svc_wake_up(svc_serv)
    唤醒svc_serv->sv_pools的第一个svc_rqst.

    16. svc_check_conn_limits(svc_serv)
    链接数不要太大,上限是svc_serv->sv_maxconn, 如果为0，改为 (svc_serv->sv_nrthreads+3)*20. 如果svc_serv->sv_tmpcnt超过这个限制,从svc_serv->sv_tempsocks队列上取出一个svc_xprt, 设置XPRT_CLOSE给svc_xprt->xpt_flags
    -> svc_xprt_enqueue(svc_xprt)

    17. svc_recv(svc_rqst, timeout)
    接受rcp request, 首先给svc_rqst分配内存，内存大小是svc_rqst->sv_max_mesg. 这么多空间大小，不能再乱了.
    -> alloc_page(GFP_KERNEL) 如果分配失败，等待500msec
    准备svc_rqst->rq_arg, rq_arg->head[0]使用第一页，rq_arg->page使用其他page,但留出一个page给reply使用. 从svc_rqst->svc_pool中去一个xprt_xprt,如果有把它和svc_rqst关联，如果没有把它放到svc_pool上.
    -> svc_xprt_dequeue(svc_pool)
    如果svc_pool上没有等待的svc_xprt,则把它放到svc_pool等待队列上，让这个任务在svc_rqst->rq_wait上等待. 睡醒的时候,svc_rqst和svc_xprt应该建立联系，如果没有把svc_rqst继续放到svc_pool上.
    检查svc_xprt->xpt_flags的XPT_CLOSE,如果它要关闭 -> svc_delete_xprt(svc_xprt)
    如果svc_xprt带有XPT_LISTENER
        -> svc_xprt->xpt_ops->xpo_accept(svc_xprt) 如果收到新的svc_xprt, 添加标志XPT_TEMP, 把它放到svc_serv->sv_tempsocks队列. 为svc_serv建立timer, 回调函数是svc_age_temp_xprts,可能是到时间把这个svc_xprt释放掉,时间是svc_conn_age_period.
        -> svc_xprt_received(svc_xprt)
    如果svc_xprt不带XPT_LISTENER标志
        -> svc_xprt->xpt_ops->xpo_has_wspace(svc_xprt)
        -> svc_deferred_dequeue(svc_xprt) 取出什么东西来?
        -> svc_deferred_recv(svc_rqst)
    -> svc_xprt_received(svc_xprt) 接受一个svc_xprt

    18. svc_drop(svc_rqst)
        -> svc_xprt_release(svc_rqst)

    19. svc_send(svc_rqst)
    这个面熟，发送数据的接口
    -> svc_xprt->svc_xprt->xpt_ops->xpo_release_rqst(svc_rqst) 发送数据之前，释放svc_rqst使用的数据. 要发送的数据总量是svc_rqst->rq_res -> head[0].iov_len + page_len, tail[0].iov_len.
    检查svc_xprt->xpt_flags的XPT_DEAD, 如果死掉返回-ENOTCONN
    -> svc_xprt->xpt_ops->ops_send(svc_rqst)
    -> rpc_wake_up(svc_xprt->xpt_bc_pending) 唤醒这个做什么?
    -> svc_xprt_release(rpc_rqst)

    20. svc_age_temp_xprts(closuere)
    timer的回调函数，关闭临时socket. 遍历svc_serv->sv_tempsocks上的所有svc_xprt, 如果没有设置XPT_OLD标志，下次再释放它，添加XPT_OLD标志. 如果svc_xprt->kref->refcount>1或带有XPT_BUSY标志,也不能释放它. 如果能释放，添加标志XPT_CLOSE, XPT_DETACHED.
    使用一下函数释放所有满足条件的svc_xprt
    -> svc_xprt_enqueue(svc_xprt)
    -> svc_xprt_put(svc_xprt)
    最后修改计时器

    21. call_xpt_users(svc_xprt)
    调用svc_xprt上所有注册的使用者回调函数，遍历svc_xprt->xpt_users, 出发svc_xpt_user->callback

    22. svc_delete_xprt(svc_xprt)
    删除svc_xprt, 它必须带标志XPT_DEAT.
    -> svc_xprt->xpt_ops->xpo_detach(svc_xprt)
    还有考虑XPT_DETACHED和svc_xprt->xpt_list. XPT_TEMP
    -> svc_deferred_dequeue(svc_xprt)
    -> call_xpt_users(svc_xprt)
    
    23. svc_close_xprt(svc_xprt)
    设置XPT_CLOSE，检查XPT_BUSY, 如果没有，删除它
    -> svc_delete_xprt(svc_xprt)

    24. svc_close_list(list_head xprt_list)
    对xprt_list上所有svc_xprt设置标志XPT_CLOSE, XPT_BUSY)

    25. svc_close_all(svc_serv)
    -> svc_close_all(svc_serv->sv_tempsocks) 把sv_tempsocks和sv_permsocks队列都删除.
    把svc_pool上的所有svc_xprt都释放, 然后释放svc_serv的两个队列
    -> svc_delete_xprt(svc_xprt)

    26. svc_resivit(cache_deferred_req, too_many)
    cache_deferred_req不熟悉

    27. svc_defer(cache_req)
    把一个请求保存起来, 返回cache_deferred_req. 检查是否可以延时处理。。。
    如果svc_rqst->rq_deferred有效,使用这个svc_deferred_req, 否则分配一个，同时要分配svc_rqst->rq_arg的内存. 把svc_rqst->rq_arg给cache_deferred_req->args.  这个svc_rqst可以不用,同时把cache_deferred_req关联svc_xprt

    28. svc_deferred_recv(svc_rqst)
    把cache_deferred_req给svc_rqst拷贝回去

    29. svc_deferred_dequeue(svc_xprt)
    svc_xprt上面有cache_deferred_req，必须带标志XPT_DEFERRED, 从svc_xprt->xpt_deferred队列上取出cache_deferred_req

    30. svc_find_xprt(svc_serv, xcl_name, net, sa_family_t, port)
    从svc_serv->sv_persocks上找一个svc_xprt,对应参数与函数参数相同.

下面是打印统计数的操作，不看了，这里看得最晕乎...
(net/sunrpc/svcsock.c)
这里该和rpcsock.c类似，但这里好像少一些,这里只看TCP实现的
svc_sock 果然比rpc_sock小
    * svc_xprt
    * socket
    * sock
    * sk_ostate(sock)
    * sk_odata(sock, bytes)
    * sk_owspace(sock)
    * sk_reclen / sk_tcplen
    * page sk_pages[RPCSVC_MAXPAGES]

    1. svc_release_skb(svc_rqst)
    svc_rqst->rq_xprt_ctx 是 sk_buff,  svc_rqst=>svc_sock
    -> skb_free_datagram_locked(svc_sock->sock, sk_buff)

    2. svc_set_cmsg_data(svc_rqst, cmsghdr)
       svc_send_common(socket, xdr_buf, page, headoffset, page, tailoffset)
       svc_sendto(svc_rqst, xdr_buf)
       svc_one_sock_name(svc_sock, buf, remaining) 
       svc_sock_names(svc_serv, buf, buflen, toclose)
        遍历svc_serv->sv_permsocks中的所有svc_xprt, 获取svc_sock, 打印ip地址和协议, 如果toclose有效，找到对应的svc_sock,把它关掉
        -> svc_one_sock_name
        -> svc_close_xprt(svc_sock->svc_xprt)

    3. svc_recv_available(svc_sock)
        -> kernel_sock_ioctl(svc_sock->sock, TIOCINQ, avail) 这是什么IOCTL？

    4. svc_recvfrom(svc_rqst, kvec, nr, buflen)
    这肯定是udp的.构造一个无所谓的msghdr
        -> kernel_recvmsg(svc_sock->socket, msg, iov, nr, buflen ...)

    5. svc_partial_recvfrom(svc_rqst, kvec, nr, buflen, base)
        接受的数据放到kvec中，但是便宜是base, 这里处理一下,把一部分kvec去掉，kvec->iov_base也修改一下.

    6. svc_sock_setbufsize(socket, snd, rcv)
        设置svc_sock->sock->sk_sndbuf / recbuf 
        svc_sock->sock->sk_write_space(svc_sock->sock)

    7. svc_udp_data_ready(sock, count)
       svc_udp_get_dest_address4(svc_rqst)
       svc_udp_get_dest_address6(svc_rqst)
       svc_udp_get_dest_address(svc_rqst)
       svc_udp_recvfrom(svc_rqst)
       svc_udp_sendto(svc_rqst)
       svc_udp_create(svc_serv, net, sockaddr, salen, flags)
       svc_udp_init(svc_sock, svc_serv)

    8. svc_write_space(sock)
        当sock有空间时，唤醒一些任务?
        sock->sk_user_data就是svc_sock, 
        -> svc_xprt_enqueue(svc_sock->svc_rqst)
        -> wake_up_interruptible(sock->sk_wq)  竟然开始使用sock的东西？


    9. svc_tcp_write_space(sock)
        -> sk_stream_wspace(sock)   >= sk_stream_min_wspace(sock)
        -> svc_write_space(sock)  同时还要去掉 sock->socket的SOCK_NOSPACE标志

    10. svc_tcp_listen_data_ready(sock, count_unused)
    监听的端口收到data_ready事件,注释说当connect建立起来时,这个函数会调用两次，父socket的data_ready和子socket的data_ready都会被调用. 这里只考虑sock->sk_state ==TCP_LISTEN的情况, sock->sk_user_data为svc_sock,设置svc_sock->svc_xprt->xpt_flags的XPT_CONN
    -> svc_xprt_enqueue(svc_xprt)
    -> wake_up_interruptible_all(sock->sk_wq->wait)

    11. svc_tcp_state_change(sock)
    svc_xprt要死掉? 设置sock=>svc_sock->svc_xprt->xpt_flags的XPT_CLOSE
    -> svc_xprt_enqueue(svc_sock->svc_xprt)
    -> wake_up_interruptible(sock->sk_wq)

    12. svc_tcp_data_ready(sock, count)
    设置svc_xprt->xpt_flags的XPT_DATA,
    -> svc_xprt_enqueue(svc_sock->svc_xprt)
    -> wake_up_interruptible(sock->sk_wq)

    13. svc_tcp_accept(svc_xprt)
    accept connect, 返回一个svc_xprt.清楚svc_xprt->xprt_flags的XPT_CONN,
    -> kernel_accept(socket, socket, O_NONBLOCK)
    设置svc_xprt->xpt_flags的XPT_CONN,
    -> kernel_getpeername(sock, sockaddr)
    -> svc_setup_socket(svc_serv, socket, err, SVC_SOCK_ANONYMOUS|SVC_SOCK_TEMPORARY) 创建一个svc_xprt.
    -> svc_xprt_set_remote(svc_sock->svc_xprt, sockaddr)
    -> kernel_getsockname(socket, sockaddr)
    -> svc_xprt_set_local(svc_sock, sockaddr)
   
    14. svc_tcp_restore_pages(svc_sock, svc_rqst)
    把svc_sock->sk_pages这些数据page给svc_rqst->rq_pages. 释放svc_rqst->rq_pages原有的. 页数根据svc_sock->sk_tcplen决定

    15. svc_tcp_save_pages(svc_sock, svc_rqst)
    把svc_rqst->rq_pages给svc_sock->sk_pages.

    16. svc_tcp_clear_pages(svc_sock)
    释放svc_sock->sk_pages使用的page

    17. svc_tcp_recv_record(svc_sock, svc_rqst)
    只接受一个record的头,也就是接受rpc_fraghdr,直接收4个字节，还要处理许多，或多次接受
    -> svc_recvfrom(svc_rqst, kvec, ...), 接受的数据量在svc_sock->sk_tcplen, 接受到rpc_fraghdr给svc_sock->sk_reclen.

    18. recevive_cb_reply(svc_sock, svc_rqst)
    svc_rqst接受到数据包，根据xid从svc_sock->svc_xprt->xpt_bc_xprt中找到rpc_rqst, 把rpc_rqst->rq_rcv_buf拷贝给rpc_rqst->rq_private_buf,把svc_rqst->rq_arg拷贝给rpc_rqst->rq_private_buf
    -> xprt_complete_rqst(rpc_rqst->rpc_task, svc_sock->sk_reclen)
   
    19. copy_pages_to_kvecs(kvec, page, len)
    把pages封装成多个kvec

    20. svc_tcp_recvfrom(svc_rqst)
    先接受头，然后接受数据.
    -> svc_tcp_recv_record(sock, svc_rqst)
    -> svc_tcp_restore_pages(svc_sock, svc_rqst)
    -> copy_pages_to_kvecs(svc_rqst->rq_vec, svc_rqst->rq_pages, svc_sock->sk_reclen)
    然后把svc_rqst->rq_pages剩余的page给svc_rqst->rq_respages.
    -> svc_partial_recvfrom(svc_rqst, vec, num ...)  
    根据svc_sock->sk_reclen,修改svc_rqst->rq_arg的长度参数. svc_rqst->rq_arg.len = svc_sock->sk_reclen
    -> receive_cb_replay(svc_sock, svc_rqst) 这是什么东西?
    -> svc_recv_available(svc_sock) 如果还有数据,设置svc_rpst->xpt_flags的XPT_DATA标志
    -> svc_xprt_copy_addrs(svc_rqst, svc_xprt)
    
    21. svc_tcp_sendto(svc_rqst)
    先封装svc_rqst->rq_res.原地封装，修改svc_rqst->rq_res->head[0]的开始4个字节.
    -> svc_sendto(svc_rqst, xdr_buf) 如果出错，好像就关闭svc_rqst
    -> svc_xprt_enqueue

    22. svc_tcp_prep_hdr(svc_rqst)
    设置svc_rqst->rq_res.head[0]的头4字节为0.

    23. svc_tcp_has_wspace(svc_xprt)
    如果svc_xprt->xpt_flags带有XPT_LISTENER, 则它是监听的,不考虑.
    -> sk_stream_wspace(svc_sock->sock) 获取它的空间？ 和svc_xprt->xpt_reserve+svc_serv->sv_max_mesg比较, 如果空间不够，设置socket->flags的SOCK_NOSPACE

    24. svc_tcp_create(svc_serv, net, sockaddr, len ...)
    -> svc_create_socket(svc_serv, IPPROTO_TCP, ...)

最后定义svc_tcp_ops,还有svc_tcp_class，下面是一些初始化函数
    1. svc_tcp_init(svc_sock, svc_serv)
    -> svc_xprt_init(svc_sock->socket->sock->net, svc_tcp_class, svc_sock->svc_xprt, svc_serv)
    设置svc_sock->svc_xprt的XPT_CACHE_AUTH. 
    如果sock->sk_state=TCP_LISTEN, 设置svc_xprt->xprt_flags的XPT_LISTENER和XPT_CONN, sock->sk_data_ready=svc_tcp_listen_data_ready. 
    否则设置三个回调函数, 还有XP_DATA. 如果sock->sk_state不是TCP_ESTABLISHED, 设置XPT_CLOSE.
    
    2. svc_sock_update_bufs(svc_serv)
    设置svc_serv->sv_permsocks和sv_tempsocks队列上所有svc_sock(svc_xprt)的XPT_CHNGBUF, 和XPT_CHANBUG 标志

    3. svc_setup_socket(svc_serv, socket, err ...)
    创建svc_sock
    -> svc_register(svc_serv, net, family, protocol, port) 向rpcbind注册, 为何svc_xprt还要注册?
    初始化svc_sock 这里的udp省掉.
    -> svc_sock_setbufsize(svc_sock->socket, 4*svc_serv->sv_max_mesg)
    -> svc_tcp_init(svc_sock, svc_serv)

    4. svc_addsock(svc_serv, fd, name, len)
    这些函数在哪里用? 添加监听端口
    -> sockfd_lookup(fd, err) 根据fd获取socket
    -> sock_setup_socket(serv_sock, sockek ...)
    -> kernel_getsockname(svc_sock->socket, sockaddr ...)
    -> svc_xprt_set_local(svc_xprt, socket ...) 根据上面获取的地址，设置本地地址，监听的端口只有本地地址, 清楚XPT_TEMP, 把svc_xprt->xpt_list加到svc_serv->sv_permsocks队列.
    -> svc_xprt_received(svc_sock->svc_xprt)

    5. svc_create_socket(svc_serv, protocol, net, sockaddr, len ...)
    -> __sock_create(net, family, type ...)
    -> kernel_bind(sock, sockaddr, len)
    -> kernel_listen(sock, 64) 监听64个?
    -> svc_setup_socket(...)

    6. svc_sock_detach(svc_xprt)
    去掉svc_sock->sock的一些回调函数
    -> wake_up_interruptible(sock->sk_sleep) 唤醒等待队列

    7. svc_tcp_sock_detach(svc_xprt)
    -> svc_sock_detach(svc_xprt)
    如果不是监听端口，还有清楚数据
    -> svc_tcp_clear_pages(svc_sock)
    -> kernel_sock_shutdown(svc_sock->socket, SHUT_RDWR)

    8. svc_sock_free(svc_xprt)
    两种情况释放，一种根据sockfd,另一种是没有sockfd时
    -> sockfd_put(svc_sock->socket)
    -> sock_release(svc_sock->socket)

最后还有backchannel的，不看了。下面是该死的整理了

还有rpcbind对应操作，对应BOUND, 这里是rpc应用，定义rpcbind call使用的rpc_program/rpc_version/rpc_procedure/rpcbind_args..
数据结构
    rpcbind_args
        * rpc_xprt
        * r_prog, r_vers, r_prot, r_port
        * r_netid, r_addr, r_owner r_owner是注册者名字
        * r_status

1. sunrpc维护rpc_clnt,向本地的rpcbind请求服务,相关操作
    a. rpcb_get_local(net)
        获取sunrpc_net, 返回sunrpc_net->rpcb_users, 应该是增加一个使用计数.

    b. rpcb_put_local(net)
        sunrpc_net数据结构中有rpcb_clnt/rpcb_clnt4, 根据计数sunrpc_net->rpcb_users判断这两个变量的使用. rpcb_clnt4是rpcbind版本
        -> rpc_shutdown_client(rpc_clnt)

    c. rpcb_set_local(net, rpc_clnt, rpc_clnt)
        应该是初始化rpcbind client时调用, 设置sunrpc_net->rpcb_local_clnt(4)

    d. rpcb_create_local_unix(net)
        构造使用AF_LOCAL协议的网络地址, path="/var/run/rpcbind.sock", rpc_create_args, 使用rpcbindV2, 创建rpc_clnt, 然后克隆一个rpc_clnt4
        -> rpc_create(rpc_create_args)
        -> rpc_bind_new_program(rpc_clnt, rpcb_program, RPCBVERS_4)
        -> rpcb_set_local(net, rpc_clnt, rpc_clnt4)
        
    e. rpcb_create_local_net(net)
        和上面一样，构造AF_INET协议的网络地址, 端口号是111, authflavor使用RPC_AUTH_UNIX,上面的使用RPC_AUTH_NULL. 这个和上面的函数参数应该不一样.

    f. rpcb_create_local(net)
        封装unix版的rpcbind使用方法, sunrpc server使用，向本地rpcbind注册服务,svc_rpcb_setup中使用.
        -> rpcb_get_local(net)
        -> rpc_create_local_unix(net)
        -> rpc_create_local_net(net) 如果unix方式的rpcbind注册不成功，使用inet的注册方式

2. 实现rpcbind的注册服务
    a. rpcb_register_call(rpc_clnt, rpc_message)
        使用rpc请求, 为下面个各样注册服务
        -> rpc_calL_sync(rpc_clnt, rpc_message, RPC_TASK_SOFTCONN)

    b. rpcb_register(net, prog, vers, prot, port)
        这里注册服务(prog/vers)使用的端口(port/prot), 为何还要指定vers? 如果port是0，使用RPCBPROC_UNSET, 否则使用RPCBPROC_SET服务. 这里构造rpcbind_args, 使用RPCBINDv2, 这里只有在RPCBINDv4不成功时才使用.
        -> rpc_register_call(rpc_local_clnt, rpc_message)

    c. rpcb_register_inet4(sunrpc_net, sockaddr, rpc_message)
        这个和上面的区别是使用RPCBINDv4, 而且rpcbind参数使用的地址是函数参数地址,同样根据sockaddr中的端口号决定使用SET/UNSET. rpcbind_args->r_addr是表示网络地址的字符串.
        -> rpcb_register_call(sunrpc_net->rpcb_local_clnt4, rpc_message)

    d. rpcb_register_inet6(sunrpc_net, sockaddr, rpc_message)
        这个和上面相通，但函数参数sockaddr是ipv6地址，也就是注册的服务地址是ipv6的

    e. rpcb_unregister_all_protofamilies(sunrpc_net, rpc_message)
        rpcbind_args->r_maps="",使用RPCBPROC_UNSET, 看来rpcbindV4使用字符串的网络地址
        -> rpcb_register_call(sunrpc_net->rpcb_local_clnt4, rpc_message)

    f. rpcb_v4_register(net, program, version, sockaddr, netid)
        这里的rpcbind_args使用netid, 而且有r_owner, 使用RPCBINDv4. 使用net的rpc_clnt,还是unix的rpc_clnt, 这个在创建上面准备rpc_clnt时确认。 这个函数在__svc_rpcb_register4中调用.
        -> rpcb_register_inet4(sunrpc_net, sockaddr, message)
        -> rpcb_register_inet6(sunrpc_net, sockaddr, message)

3. 实现rpcbind的服务查询，实现rpc_xprt bind操作
    a. rpcb_wake_rpcbind_waiters(rpc_xprt, status)
        清除rpc_xprt的XPRT_BINDING标志，唤醒rpc_xprt->binding上的任务
        -> rpc_wake_up_status(rpc_xprt->binding, status)

    b. rpcb_map_release(data)
        data是rpcbind_args, 唤醒rpc_xprt. 这是rpc_call_ops->rpc_release
        -> rpcb_wake_rpcbind_waiter(rpcbind_args->r_xprt, rpcbind_args->r_status)
        释放rpcbind_args, rpcbind_args->r_addr

    c. rpcb_create(net, hostname, sockaddr, salen, proto, version)
        构造一个rpc_clnt使用服务地址(ip/port, protocol),还有请求地址(rpc_prog,rpc_ver), 但这里可以确认rpc_prog,rpc_vers可能使用RPCBINDv2/4. 创建一个请求rpcbind服务的rpc_clnt.
        -> rpc_create(rpc_create_args)


    d. rpcb_call_async(rpc_clnt, rpc_bind_args, rpc_procinfo)
        构造rpc_message, rpc_task_setup, 都忘了rpc_task_setup作用了, 使用RPC异步执行方式,返回rpc_task. rpc client 查询端口使用异步方式, server注册服务使用同步方式.
        -> rpc_run_task(rpc_task_setup)

    e. rpcb_find_transport_owner(rpc_clnt)
        找到rpc_clnt, rpc_clnt根据clone产生，向上找rpc_clnt->cl_parent, 直到rpc_xprt不同,或设置autobind的rpc_clnt.

    f. rpcb_getport_async(rpc_task)
        这是请求rpcbind服务，根据rpc_task中的server ip, rpc_prog/ver,找到server port. 这是FSM过程中使用的. 上面的register函数是rpc server使用的注册函数，这里只需要GETPORT请求. 这是call_bound的实现.
        -> rpcb_find_transport_owner(rpc_task->rpc_clnt)
        -> rpc_sleep_on(rpc_clnt->rpc_xprt->binding, rpc_task, NULL)
        -> xprt_test_and_set_binding / xprt_bound(rpc_xprt) 如果设置XPRT_BINDING,有人在做bound的工作,直接返回. 然后检查XPRT_BOUND,是否已经完成BOUND工作. 
        -> rpc_peeraddr(rpc_clnt, ...) 从rpc_clnt获取server地址
        -> rpcb_create(rpc_xprt->net, rpc_clnt->cl_server, rpc_xprt->prot, bind_version)
        -> 找到GETPORT proc/rpcbind_version版本，对应ipv4/ipv6, 构造rpcbind_args, 填充rpc_prog/vers/protocol/netid/addr, 他还是用rpc_xprt?
        -> rpc_call_async(rpc_clnt, rpcbind_args, proc) 调用rpcbind服务
        -> rpc_release_client(rpc_clnt)
        -> rpc_put_task(rpc_task)
        -> rpcb_wake_rpcbind_waiters(rpc_xprt, status) 错误处理,发现XPRT_BOUND有效.

    g. rpcb_getport_done(rpc_task, data)
        这是rpc_call_ops->rpc_call_done函数，在rpc_exit中调用, 这样调用rpc请求后，不用处理结果，在回调函数中自动设置rpcbind_args->rpc_xprt.
        -> rpc_xprt->rpc_xprt_ops->set_port(rpc_xprt, rpcbind_args->r_port) rpcbind_args并不仅仅是xdr_buf.

4. 下面是xdr_encode_func_t之类的函数，还有rpc_procedure信息
    a. rpcb_enc_mapping(rpc_rqst, xdr_stream, rpcbind_args)
        xdr函数，编码rpcbind_args参数

    b. rpcb_dec_getport(rpc_rqst, xdr_stream, rpcbind_args) / rpcb_dec_set / encode_rpcb_string(xdr_stream, string, maxstrlen) / rpcb_enc_getaddr(rpc_rqst, xdr_stream, rpcbind_args)
        rpc_procinfo rpcb_procedure2/3/4

这里的功能模块还是比较简单，主要提供rpcbind相关的服务接口,分别为server和client服务，为server服务时，需要对本地rpcbind做注册工作，以及注销工作。对client的服务工作是查询port, 这是FSM的一部分，对应call_bind操作, 而且不会操作很多, 一次查询就可以. 通过这里学习了如果使用rpc client功能,创建rpc_clnt, rpc_task, 调用rpc_task. rpc_xprt的创建集成到了rpc_clnt中。

