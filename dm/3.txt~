dm-io.c

dm_io_client
  * mempool_t pool
  * bio_set bios  

region是一段io数据,对应dm_io_region和dm_io_request

io
  * error_bits   
  * count
  * task_struct sleeper
  * dm_io_client client
  * io_notify_fn callback
  * context
  * vma_invalidate_address
  * vm_invalidate_size


* store_io_and_region_in_bio(bio, io, region)
  # 把region和io一块放到bio->bi_private

* retrieve_io_and_region_from_bio(bio, io, region)
  #获取io和region.  BM_IO_MAX_REGIONS

* dec_count(io, region, error)
  # io的第region部分完成,记录结果.  如果io->count减为0, 释放io??
  > invalidate_kernel_vmap_range(io->vma_invalidate_address, io->vma_invalidate_size)
  > mempool_free(io, io->client->pool)
  > io->callback(io->context)

* endio(bio, error)
  # bio的回调函数
  > retrieve_io_and_region_from_bio(bio, io, region)
  > bio_put(bio)
  > dec_count(io, region, err)

dpages
  * get_page(dpages, page, len, offset)
  * next_page(dpages)
  * context_u, context_ptr
  * vma_invalidate_address,  vma_invalidate_size

dm_io_region
  * block_device bdev
  * sector, count
从下面看这两个是一块用的,dpages表示io数据的内存位置, dm_io_region表示磁盘位置..

page_list
  * page_list next 
  * page

dm_io_mem_type
  * dm_io_mem_type   #PAGE_LIST/BVEC/VMA/KMEM, 分别对应最后四种指针,union的四个成员.
  * offset 
  * page_list
  * bvec 
  * vma 
  * addr 

dm_io_request
  * bi_rw
  * dm_io_memory mem 
  * dm_io_notify  notify
  * dm_io_client  client


dpages是遍历page_list的辅助结构..
* list_get_page(dpages, page, len, offset)
  #获取dpages中的信息page,  countext_ptr是page_list, context_u是offset

* list_next_page(dpages)
  #context_ptr是page_list, 可使用context_ptr遍历..

上面是page_list, 下面是bio_vec, context_ptr是bio_vec..
* list_dp_init(dpages, page_list, offset)
  #构造dpages,  context_ptr指向bio_vec数组..

* bvec_get_page(dpages, page, len, off)
  # 

* bvec_next_page(dpage)
  #

* bvec_dp_init(dpages, bio_vec)
  # 

dpages里面是vmalloc指针, 需要转换为page
* vm_get_page(dpages, page, len, offset) 
  > virt_to_page(void*)
* vm_next_page(dpages)

* vm_dp_init(dpages, data)

dpages里面是地址指针,不过是kmalloc的..
km_get_page(dpages, pages, len, offset)
  > virt_to_page(dp->context_ptr)

km_next_page(dpages)
km_dp_init(dp, data)

* do_region(rw, region, dm_io_region, dpages, io)
  # io数据在dm_io_region中,根据get_page获取page. 先创建bio, 把page放到io中. 回调函数是endio, 释放io.
  > bio_alloc_bioset(GFP_NOIO, num_bvecs, io->client->bios)
    # io里面是哪里给的??
  > store_io_and_region_in_bio(bio, io, region)
  > bio_add_page(bio, page, logical_block_size, ..)
  > dpages->get_page(...)  / dpages->next_page(...)
  > submit_bio

* dispatch_io(rw, num_regions, dm_io_region, dpages, io, sync)
  # 第三个参数是dm_io_region数组, 提交io., num_regions对应error_flags的error的个数..
  > do_region(...)

* sync_io(dm_io_client, num_regions, dm_io_region, rw, dpages, error_bits)
  # 这里没有io, region,dpages也需要吗?? 先构造io, error_bits是错误结果?? sleeper是current, client是参数dm_io_client 
  > dispatch_io(rw, num_regions, where, dp, io, 1)
    #提交io..然后等待io->count, 最后检查io->error_bits

* async_io(dm_io_client, num_regions, dm_io_region, rw, dp, fn, context)
  # 这里是异步,在mempools中分配io, 设置回调函数,fn/context等等..
  > mempool_alloc(dm_io_client->pool, GFP_NOIO)
  > dispatch_io(rw, num_regions, where, dpages, io, 0)

* dp_init(dm_io_request, dpages, size)
  # 根据dm_io_request->mem.type初始化dpages..
  > list_dp_init
  > bvec_dp_init

* do_io(dm_io_region, num_regions, dm_io_region, sync_error_bits)
  > dp_init(..
  > sync_io(...
  > async_io(..
    # 如果dm_io_region->notify.fh有意义,就是async调用.


================================
dm_io
  mapped_device md
  int error
  atomic_t io_count 
  bio bio #这里是指针,释放时不干扰bio
  long start_time 
  spinlock_t  endio_lock

dm_target_io
  dm_io   io
  dm_target 
  map_info info 
  target_request_nr..
  bio clone  #这是嵌套不是指针

dm_rq_target_io
  mapped_device md 
  dm_target  ti 
  request  orig, clone  ; # 好久没看到这个request..
  error
  map_info  info

map_info 
  void *ptr 
  long long ll

dm_rq_clone_bio_info 
  bio orig   #指针
  dm_rq_target_io tio #回指, 它在bio->bi_private
  bio clone  #嵌套


dm_io是给bio_based的设备使用的,  dm_rq_target_io是request_based的设备使用的.. 为何看不出什么关系??

mapped_device 
  rw_semaphore io_lock
  mutex suspend_lock 
  rwlock_t  map_lock 
  atomic_t holders  #使用计数
  open_count   #open计数
  flags 
  request_queue queue 
  type 
  mutex type_lock 
  target_type  immutable_target_type 
  gendisk disk
  name 
  interface_ptr
  pending   #request计数..
  wait_queue_head_t wait  #等待的请求
  work_struct work     #bio的执行函数是什么??
  bio_list  deferred   #bio队列
  spinlock_t  defferred_lock 

  workqueue_struct wq  #bio的工作队列??

  dm_table  map 
  mempool_t io_pool,  tio_pool 

  bio_set bs 

  atomic_t event_nr    #怎么使用uevent
  wait_queue_head_t eventq 
  atomic_t uevent_seq 
  list_head uevent_list 
  uevent_lock
  
  super_block frozen_sb  #sb??
  block_device bdev
  
  hd_geometry geometry
  kobject

  bio  flush_bio

虽然这个数据结构很大,但也比md好多了. md 应该赶紧删除

dm_init, 高科技，还定义函数指针数组, 初始化target, linear, stripe, io, kcopyd, interface...

* dm_blk_open(block_device, fmode_t)
  #block_dev->gendisk->private_data是mapped_device, 通过mapped_device管理使用计数.. gendisk是在注册设备时创建,而且是全局的.  最后还要增加mapped_device->open_count
  > dm_get(mapped_device)  
  
* dm_blk_close(gendisk, fmode_t)

* dm_blk_getgeo( block_device, hd_genmetry)
  # 获取mapped_device->geometry..
  > dm_get_geometry(mapped_deivce, hd_geometry)

* dm_blk_ioctl(block_device, fmode_t, cmd, arg)
  > dm_table_get_num_targes(dm_table) 
    # 只支持只有一个dm_target的情况..
  > dm_suspended_md(mapped_device)
  > dm_target->target_type->ioctl(...)

* queue_io(mapped_device, bio)
  # mapped_device->deferred是bio队列
  > queue_work(mapped_device->workqueue_struct, mapped_device->work_struct)

* dm_get_live_table(mapped_device)
  #获取mapped_device->dm_table

* dec_pending(dm_io, error)
  #mapped_device->flags的MDF_NOFLUSH_SUSPENDING什么意思?? 这个函数好像是bio回调函数,减小dm_io->io_count,如果为0,处理error.  如果为DM_ENDIO_REQUEUE, 把它放到mapped_device->deferred队列中, 而且没有刷新工作要做...
  #从dm_io中获取bio, 释放dm_io,dm_io应该是bio的一个包装,dm_io的工作完成, bio应该也完成了,如果bio是REQ_FLUSH,提交bio,否则完成bio
  > queue_io(mapped_device, bio)
  > bio_endio(bio, io_error) 

* clone_endio(bio, error)
  # bio->bi_private是dm_target_io, 管理分裂的bio??
  > dm_target_io->dm_target->target_type->end_io(dm_target_io->dm_target, bio, error)
  > free_bio(mapped_device, dm_target_io)
    #仅仅是释放dm_target_io->clone 嵌套的bio, 它也是一个bio..
  > dec_pending(dm_io, error)

* end_clone_bio(bio, error)
  #bio->bi_private是dm_rq_clone_bio_info, 这个request_based使用的..保存error结果到dm_rq_target_io->error.  应该是dm_rq_clone_bio_info->orig/bio完成，告诉dm_rq_target_io
  > blk_update_request(dm_rq_target_io->orig, 0, nr_bytes)

* rq_completed(mapped_device, rw, run_queue)
  > md_in_flight(mapped_device)
  > wakeup(mapped_device->wait)
    # 如果有等待的request, 唤醒等待的任务
  > blk_run_queue_async(mapped_device->queue)
    #启动request提交???
  > dm_put(mapped_deivce)

* free_rq_clone(request)
  # 释放dm_rq_target_io, 它就是request->end_io_data, 释放dm_rq_target_io???
  > blk_rq_unprep_clone(request)
    # 释放request中的所有bio..
  > free_rq_tio(dm_rq_target_io)
    # 释放自我..

* dm_end_request(request, error)
  # request->end_io_data是dm_rq_target_io, 里面有dm_target, 还有orig/request, 
  > free_rq_clone(request)
  > blk_end_request_all(request, error)
    # 这是报告完成结果, request里面也会释放bio..
  > rq_completed(mapped_deivce, rw, true)

* dm_unprep_request(request)
  > free_rq_clone(request/clone)

* dm_requeue_unmapped_request(request)
  # 这个request是clone的, 获取dm_rq_target_io, 
  > dm_unprep_request(request)
    #把clone的request释放掉
  > blk_requeue_request(request_queue, request)
    #把dm_rq_target_io->orig/request放到队列中
  > rq_completed(mapped_deivce, rw, 0)
    #启动request的处理???

* stop_queue(request_queue)
  > __stop_queue(request_queue) 
    > blk_stop_queue(request_queue)
      # 取消workqueue..

* start_queue(request_queue)
  # 就是启动request_queue...
  > blk_start_queue(request_queue)
    > __blk_run_queue(request_queue)

* dm_done(request, error, mapped)
  #处理clone的request,  dm_rq_target_io, dm_target=>target_type
  > target_type->rq_end_io(dm_target, request, error, dm_rq_target_io->map_info
  > dm_end_request(request, error)  
    #如果返回<=0 ...
  > dm_requeue_unmapped_request(request)
    # 如果返回DM_ENDIO_REQUEUE, 重新处理request..

* dm_softirq_done(request)
  > dm_one(request, dm_rq_target_io->error, mapped)

* dm_complete_request(request, error)
  # 回调函数?? 根据request获取dm_rq_target_io, 还有原来的request, 设置dm_rq_target_io->error, request->completion_data=clone/request
  > blk_complete_request(request)
    # 这个函数很怪，启动softirq,BLOCK_SOFTIRQ

* dm_kill_unmapped_request(request, error)
  > dm_complete_request(request, error)
    #设置request->cm_flags的REQ_FAILED标志, 哪里使用它?  上面会结束request??

* end_clone_reques(request, error)
  > __blk_put_request(request->request_queue, request) 
  > dm_complete_request(request, error)

* max_io_len_target_boundary(sector, dm_target)
  # dm_target->len - (sector - dm_target->start)...

* max_io_len(sector, dm_target)
  #检查sector是否合适?  还是sector开始最长io???

* dm_set_target_max_io_len(dm_target, len)
  # 设置 dm_target->max_io_len ...

* __map_bio(dm_target, dm_target_io)
  # 处理clone bio..  设置bio->bi_end_io = clone_endio, bi_private是dm_target_io
  > dm_target->target_type->map(dm_target, bio)
  > generic_make_request(bio)
    # 如果map没问题,提交bio
  > dec_pending(dm_target_io->dm_io->mapped_device, r)
  > free_tio(mapped_deivce, dm_target_io)

* split_bvec(dm_target_io, bio, sector, idx, offset, len, bio_set)
  # 构造一个bio,完成一部分bvec..
